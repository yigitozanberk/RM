---
title: "Regression Modelling Week 2"
author: "Yigit Ozan Berk"
date: "7/10/2019"
output: html_document
---

# Statistical Linear Regression Models

Up to this point, we’ve only considered estimation. Estimation is useful, but we also need to know how to extend our estimates to a population. This is the process of statistical inference. Our approach to statistical inference will be through a statistical model. At the bare minimum, we need a few distributional assumptions on the errors. However, we’ll focus on full model assumptions under Gaussianity.


notes on the paper sheet

## linear regression for prediction

example from 'diamond' data set from UsingR

```{r}
library(UsingR)
data(diamond)
library(ggplot2)
g <- ggplot(diamond, aes(x = carat, y = price))
g <- g + xlab("Mass (carats)")
g <- g + ylab("Price (SIN $)")
g <- g + geom_point(size = 6, colour = "black", alpha = 0.2)
g <- g + geom_point(size = 5, colour = "blue", alpha = 0.2)
#black background and blue top colour
g <- g + geom_smooth(method = "lm", colour = "black")
#assumes y as outcome, x as predictor
g

```

```{r}
fit <- lm(price ~ carat, data = diamond)
coef(fit)
```

we estimate an expected `r round(coef(fit)[2], 2)` (SIN) $ for every carat increase in mass of diamond.

```{r}
summary(fit)
```

```{r}
#mean centering
fit2 <- lm(price ~ I(carat - mean(carat)), data = diamond)
#if you want mathmatical functions within lm function, you have to surround them in the I() function
coef(fit2)
```

500 SIN $ is the average price for the average sized diamond which is about X = 0.2.

```{r}
#changing units of slope to 1/10 of a carat
fit3 <- lm(price ~ I(carat * 10), data = diamond)
coef(fit3)
```

```{r}
#predicting the price
newx <- c(0.16, 0.27, 0.34)
coef(fit)[1] + coef(fit)[2]*newx

```
```{r}
predict(fit, newdata = data.frame(carat = newx))
#takes output from several different prediction models
# if you don't use the newdata variable, it predicts yhat values of the original x values in the model 'fit'
```


# Residuals

Residuals represent variation left unexplained by our model. We emphasize the difference between residuals and errors. The errors unobservable true errors from the known coefficients, while residuals are the observable errors from the estimated coefficients. In a sense, the residuals are estimates of the errors.

Diamond data set from UsingR

```{r}
library(UsingR)
data(diamond)
library(ggplot2)

g <- ggplot(diamond, aes(x = carat, y = price))
g <- g + xlab("Mass (carats)")
g <- g + ylab("Price (SIN $)")
g <- g + geom_point(size = 6, colour = "black", alpha = 0.2)
g <- g + geom_point(size = 5, colour = "blue", alpha = 0.2)
#black background and blue top colour
g <- g + geom_smooth(method = "lm", colour = "black")
#assumes y as outcome, x as predictor
g
```

The variation around the regression line is the residual variation.

The distances that make up the residual variation are called the residuals

notes on papersheet

```{r}
y <- diamond$price; x <- diamond$carat ; n <- length(y)
fit <- lm(y ~ x)
e <- resid(fit)
yhat <- predict(fit)
#ifyou don't give the prediction function no new data, it will predict at the observed x values.
max(abs(e - (y - yhat)))
max(abs(e - (y - coef(fit)[1] - coef(fit)[2]*x)))
```

```{r}
sum(e)
#close enough to zero
```

```{r}
sum(e * x)
#close enough to zero
```

```{r}
plot(diamond$carat, diamond$price,
     xlab = "Mass (carats)",
     ylab = "Price( SIN $",
     bg = "lightblue",
     col = "black", cex = 1.1, pch = 21, frame = FALSE)
abline(fit, lwd = 2)
for(i in 1 : n)
        lines(c(x[i], x[i]), c(y[i], yhat[i]), col = "red", lwd = 2)
```


my residuals are the red lines. the distances

if point is above the residual is positive, if below the residual is negative.

plotting residuals ~ mass

```{r}
plot(x, e,
     xlab = "Mass(carats)",
     ylab = "Residuals(SIN $)",
     bg = "lightblue",
     col = "black", cex =2, pch = 21, frame = F)
abline(h = 0, lwd = 2)
for (i in 1:n)
        lines(c(x[i], x[i]), c(e[i], 0), col = "red", lwd = 2)
```

from this plot we can see that there were a lot of diamonds with the same mass.



```{r}
x <- runif(100, -3, 3); y <- x + sin(x)+ rnorm(100, sd = .2);
g <- ggplot(data.frame(x = x, y = y), aes(x = x , y = y))
g <- g + geom_smooth(method = "lm", colour = "black")
g <- g + geom_point(size = 7, colour = "black", alpha = 0.4)
g <- g + geom_point(size = 5, colour = "red", alpha = 0.4)
g
```


this is not the correct model for this data.

there is a linear trend in the data, but the line is not accounting only for the non-linear trend in the data.

you can get meaningful information about trends from incorrect models. the goal is not finding the exact correct model in regression.

```{r}
g <- ggplot(data.frame(x =x , y = resid(lm(y ~ x))), aes(x = x, y = y))
g <- g + geom_hline(yintercept = 0, size = 2)
g <- g + geom_point(size = 7, colour = "black", alpha = 0.4)
g <- g + geom_point(size = 5, colour = "red", alpha = 0.4)
g <- g + xlab("X") + ylab("Residual")
g
```



now the sin trend is extremely apparent

```{r}
# Heteroskedasticity
x <- runif(100, 0 , 6); y <- x + rnorm(100, mean = 0, sd = 0.001*x);
g <- ggplot(data.frame(x = x, y = y), aes(x = x , y = y))
g <- g + geom_smooth(method= "lm", colour = "black")
g <- g + geom_point(size  =7, colour = "black", alpha = 0.4)
g <- g + geom_point(size = 5, colour = "red", alpha = 0.4)
g
```

difficult to see in the plot.

```{r}
#getting rid of the blank space can be helpful
g <- ggplot(data.frame(x = x, y = resid(lm(y ~ x))),
            aes(x = x, y = y))
g <- g + geom_hline(yintercept = 0, size  =2)
g <- g + geom_point(size  = 7, colour = "black", alpha = 0.4)
g <- g + geom_point(size =  5, colour = "red", alpha = 0.4)
g <- g + xlab("X") + ylab( "Residual")
g
```

here it is apparent.

heteroskedasticity is one of those things residuals plot is finding out

you couldn't see it on the regression plot.

```{r}
#let's add the residual plot of the diamond data
diamond$e <- resid(lm(price ~ carat , data = diamond))
g <- ggplot(diamond, aes(x = carat, y = e))
g <- g + xlab("Mass (carats)")
g <- g + ylab("Residual price (SIN$")
g <- g + geom_hline(yintercept = 0, size = 2)
g <- g + geom_point(size = 7, colour = "black", alpha = 0.5)
g <- g + geom_point(size = 5, colour = "blue", alpha = 0.2)
g
```

residuals have the same unit as the outcome

there doesn't appear a pattern. it's good. looks random.



```{r}
e <- c(resid(lm(price ~ 1, data = diamond)),
       resid(lm(price ~ carat, data = diamond)))
#first residual vector is the one where you just fit in an intercept
#residuals are the deviatios from the average price
#second one is the variation around the regression line.
fit <- factor(c(rep("Itc", nrow(diamond)),
                rep("Itc, slope", nrow(diamond))))
#factor variable that labels the set of residuals
g <- ggplot(data.frame(e = e, fit = fit), aes(y = e, x = fit, fill = fit))
g <- g + geom_dotplot(binaxis = "y", size = 2, stackdir = "center", binwidth =  20)
g<- g + xlab("Fitting approach")
g <- g + ylab("Residual price")
g

```


# Residual variance

```{r}
data(diamond)
y <- diamond$price; x<- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
summary(fit)$sigma
#residual st dev estimate
```

```{r}
sqrt(sum(resid(fit)^2)/(n - 2))
#confirmed with 46 degrees of freedom
```
R squared can be a misleading summary of model fit.


```{r}
data(anscombe)
example(anscombe)
```

same mean and variance of X and Y
identical correlations(hence same R^2)
same linear regression relationship.

BUT the models are significantly different.

1st plot - just a normal x and y relationship and regression.
2nd plot - there's clearly a missing term to explain the curvature
3rd plot - there's an outlier
4th plot - all data are at one value, and there is another value at the far end



# Inference in Regression

Inference is the process of drawing conclusions about a population using a sample. In statistical inference, we must account for the uncertainty in our estimates in a principled way. Hypothesis tests and confidence intervals are among the most common forms of statistical inference.

These statements apply generally, and, of course, to the regression setting that we've been studying. In the next few lectures, we'll cover inference in regression where we make some Gaussian assumptions about the errors.

```{r}
library(UsingR); data(diamond)
y <- diamond$price; x<- diamond$carat; n <- length(y)
beta1 <- cor(y,x)*sd(y)/sd(x)
#theoretical beta1
beta0 <- mean(y) - beta1 *mean(x)
#theoretical beta0
e <- y - beta0 - beta1*x
#residuals
sigma <- sqrt(sum(e^2)/(n-2))
#variability around the regression line
ssx <- sum((x - mean(x))^2)
#sums of squares of xs
seBeta0 <- (1/n + mean(x)^2 / ssx) ^ .5 * sigma
#standard error for Beta0
seBeta1 <- sigma/sqrt(ssx)
#standard error for Beta1
tBeta0 <- beta0 / seBeta0
#divided by its standard error
#t statistic that beta1 is zero or beta0 is zero
#we don't have to subtract the true value because the true value is accepted as zero under this hypothesis
#test of a 0 intercept. usually not very interesting
tBeta1 <- beta1 / seBeta1
#divided by its standard error
#t statistic that beta1 is zero or beta0 is zero
#we don't have to subtract the true value because the true value is accepted as zero under this hypothesis
#test of a 0 slope. this is the important part
pBeta0 <- 2 * pt(abs(tBeta0), df = n-2, lower.tail = FALSE)
#twice my t probability
pBeta1 <- 2 * pt(abs(tBeta1), df = n-2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std.Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
coefTable
```

Note : 
The t statistic is a measure of how extreme a statistical estimate is. You compute this statistic by subtracting the hypothesized value from the statistical estimate and then dividing by the estimated standard error. In many, but not all situation, the hypothesized value would be zero.

You have an indication that the hypothesized value is reasonable when the t-statistic is close to zero.  Alternately, you have an indication that the hypothesized value is not large enough when the t-statistic is large positive. Finally, you have an indication that the hypothesized value is too large when the t-statistic is large negative.

```{r}
coefTable
fit <- lm(y ~ x)
summary(fit)$coefficients
```

```{r}
summary(fit)
```

confidence intervals for our intercept and slope

```{r}
sumCoef <- summary(fit)$coefficients
sumCoef
```


confidence intervals
```{r}
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1,2]
#estimate +- .975th t quantile * st. error
sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2,2]
#estimate +- .975th t quantile * st. error

```

the slope is going to be interpreted in change in sNG dollar price per one mass increase in carat. But I might want it for a 0.1 increase. 1 carat increase is a big increase

```{r}
(sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1,2]) / 10
#estimate +- .975th t quantile * st. error / 10 (for slope * 0.1 units)
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2,2]) / 10
#estimate +- .975th t quantile * st. error / 10 (for slope * 0.1 units)
```

with 95% confidence we estimate that a 0.1 carat increase in diamond size is going to result in 355 to 388 increase in price in SNG $.

# Prediction

Prediction is a central concept for the data scientist. 
The entire course, Practical Machine Learning is on advanced prediction techniques.

```{r}
library(ggplot2)
newx <- data.frame(x = seq(min(x), max(x), length = 100))
p1 <- data.frame(predict(fit, newdata = newx, interval = ("confidence")))
#the predict function has the output of lm. some new x values(more dense). interval = ("confidence") means: "I want the interval around the estimated line at that particular value of x. not for a potential new y at that particular value of x.
# if you want the particular values of y around the estimated line, then you pick interval = ("prediction")
p2 <- data.frame(predict(fit, newdata = newx, interval = ("prediction")))
p1$interval = "confidence"
p2$interval = "prediction"
p1$x = newx$x
p2$x = newx$x
dat = rbind(p1, p2)
names(dat)[1] = "y"

g = ggplot(dat, aes(x = x, y = y))
g = g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2)
#ribbon has 2 parts. because it is going to be filled by the interval types. blue is the prediction interval
g = g + geom_line()
#the fitted line
g = g + geom_point(data = data.frame(x = x, y =y), aes(x = x, y = y), size = 4)
#observed data points are not in the data frame that I used to create the ggplot, so I have to give it a new data frame and a new aes
g
```


salmon is the confidence interval for the line. blue is the prediction interval. 

ozet = the leftover residual variation of my regression line is kinda the main source of the prediction interval being larger than the confidence interval. there's some uncertainty that doesn't go away with better estimation.

# Swirl

- Residual Variation
- Introduction to Multivariable Regression
- MultiVar Examples

## Residual Variation

As shown in the slides, residuals are useful for indicating how well data
| points fit a statistical model. They "can be thought of as the outcome (Y)
| with the linear association of the predictor (X) removed. One differentiates
| residual variation (variation after removing the predictor) from systematic
| variation (variation explained by the regression model)."

...

  |======                                                                 |   9%
| It can also be shown that, given a model, the maximum likelihood estimate of
| the variance of the random error is the average squared residual. However,
| since our linear model with one predictor requires two parameters we have only
| (n-2) degrees of freedom. Therefore, to calculate an "average" squared
| residual to estimate the variance we use the formula 1/(n-2) * (the sum of the
| squared residuals). If we divided the sum of the squared residuals by n,
| instead of n-2, the result would give a biased estimate.


First, we'll use the residuals (fit$residuals) of our model to estimate the
| standard deviation (sigma) of the error. We've already defined n for you as
| the number of points in Galton's dataset (928).

 First, we'll use the residuals (fit$residuals) of our model to estimate the
| standard deviation (sigma) of the error. We've already defined n for you as
| the number of points in Galton's dataset (928).

...

  |================                                                       |  23%
| Calculate the sum of the squared residuals divided by the quantity (n-2).
| Then take the square root.

> sqrt(sum(fit$residuals^2)/(n-2))
[1] 2.238547

| That's correct!

  |===================                                                    |  27%
| Now look at the "sigma" portion of the summary of fit, "summary(fit)$sigma".

> summary(fit)$sigma
[1] 2.238547

| That's the answer I was looking for.
Another cool thing - take the sqrt of "deviance(fit)/(n-2)" at the R prompt.

> deviance(fit)/(n-2)
[1] 5.011094

| That's not the answer I was looking for, but try again. Or, type info() for
| more options.

| Type "sqrt(deviance(fit)/(n-2))" at the R prompt.

> sqrt(deviance(fit)/(n-2))
[1] 2.238547

| That's correct!

Another useful fact shown in the slides was

...

  |================================                                       |  45%
| Total Variation = Residual Variation + Regression Variation

Recall the beauty of the slide full of algebra which proved this fact. It had
| a bunch of Y's, some with hats and some with bars and several summations of
| squared values. The Y's with hats were the estimates provided by the model.
| (They were on the regression line.) The Y with the bar was the mean or average
| of the data. Which sum of squared term represented Total Variation?

1: Yi_hat-mean(Yi)
2: Yi-mean(Yi)
3: Yi-Yi_hat

Selection: 2

| Keep up the great work!

  |=======================================                                |  55%
| Which sum of squared term represents Residual Variation?

1: Yi_hat-mean(Yi)
2: Yi-Yi_hat
3: Yi-mean(Yi)

Selection: 2

| Excellent job!

  |==========================================                             |  59%
| The term R^2 represents the percent of total variation described by the model,
| the regression variation (the term we didn't ask about in the preceding
| multiple choice questions). Also, since it is a percent we need a ratio or
| fraction of sums of squares. Let's do this now for our Galton data.


Recall that centering data means subtracting the mean from each data point.
| Now calculate the sum of the squares of the centered children's heights and
| store the result in a variable called sTot. This represents the Total
| Variation of the data.

> sTot <- sum((galton$child - mean(galton$child))^2)

| That's not the answer I was looking for, but try again. Or, type info() for
| more options.

| Type "sTot <- sum((galton$child-mu)^2)" at the R prompt.

> sTot <- sum((galton$child - mu)^2)

| You nailed it! Good job!

  |====================================================                   |  73%
| Now create the variable sRes. Use the R function deviance to calculate the sum
| of the squares of the residuals. These are the distances between the
| children's heights and the regression line. This represents the Residual
| Variation.


Type "sRes <- deviance(fit)" at the R prompt.

> sRes <- deviance(fit)

| You got it right!

  |=======================================================                |  77%
| Finally, the ratio sRes/sTot represents the percent of total variation
| contributed by the residuals. To find the percent contributed by the model,
| i.e., the regression variation, subtract the fraction sRes/sTot from 1.  This
| is the value R^2.


Finally, the ratio sRes/sTot represents the percent of total variation
| contributed by the residuals. To find the percent contributed by the model,
| i.e., the regression variation, subtract the fraction sRes/sTot from 1.  This
| is the value R^2.

> 1 - sRes/sTot
[1] 0.2104629

| You are doing so well!

  |==========================================================             |  82%
| For fun you can compare your result to the values shown in
| summary(fit)$r.squared to see if it looks familiar. Do this now.

summary(fit)$r.squared
[1] 0.2104629

| That's correct!

  |=============================================================          |  86%
| To see some real magic, compute the square of the correlation of the galton
| data, the children and parents. Use the R function cor.



To see some real magic, compute the square of the correlation of the galton
| data, the children and parents. Use the R function cor.

> cor(galton$child, galton$parent)^2
[1] 0.2104629

| Perseverance, that's the answer.

  |=================================================================      |  91%
| We'll now summarize useful facts about R^2. It is the percentage of variation
| explained by the regression model. As a percentage it is between 0 and 1. It
| also equals the sample correlation squared. However, R^2 doesn't tell the
| whole story.



# Introduction to Multivariable Regression



