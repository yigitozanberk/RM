---
title: "Regression Modelling Week 2"
author: "Yigit Ozan Berk"
date: "7/10/2019"
output: html_document
---

# Statistical Linear Regression Models

Up to this point, we’ve only considered estimation. Estimation is useful, but we also need to know how to extend our estimates to a population. This is the process of statistical inference. Our approach to statistical inference will be through a statistical model. At the bare minimum, we need a few distributional assumptions on the errors. However, we’ll focus on full model assumptions under Gaussianity.


notes on the paper sheet

## linear regression for prediction

example from 'diamond' data set from UsingR

```{r}
library(UsingR)
data(diamond)
library(ggplot2)
g <- ggplot(diamond, aes(x = carat, y = price))
g <- g + xlab("Mass (carats)")
g <- g + ylab("Price (SIN $)")
g <- g + geom_point(size = 6, colour = "black", alpha = 0.2)
g <- g + geom_point(size = 5, colour = "blue", alpha = 0.2)
#black background and blue top colour
g <- g + geom_smooth(method = "lm", colour = "black")
#assumes y as outcome, x as predictor
g

```

```{r}
fit <- lm(price ~ carat, data = diamond)
coef(fit)
```

we estimate an expected `r round(coef(fit)[2], 2)` (SIN) $ for every carat increase in mass of diamond.

```{r}
summary(fit)
```

```{r}
#mean centering
fit2 <- lm(price ~ I(carat - mean(carat)), data = diamond)
#if you want mathmatical functions within lm function, you have to surround them in the I() function
coef(fit2)
```

500 SIN $ is the average price for the average sized diamond which is about X = 0.2.

```{r}
#changing units of slope to 1/10 of a carat
fit3 <- lm(price ~ I(carat * 10), data = diamond)
coef(fit3)
```

```{r}
#predicting the price
newx <- c(0.16, 0.27, 0.34)
coef(fit)[1] + coef(fit)[2]*newx

```
```{r}
predict(fit, newdata = data.frame(carat = newx))
#takes output from several different prediction models
# if you don't use the newdata variable, it predicts yhat values of the original x values in the model 'fit'
```


# Residuals

Residuals represent variation left unexplained by our model. We emphasize the difference between residuals and errors. The errors unobservable true errors from the known coefficients, while residuals are the observable errors from the estimated coefficients. In a sense, the residuals are estimates of the errors.

Diamond data set from UsingR

```{r}
library(UsingR)
data(diamond)
library(ggplot2)

g <- ggplot(diamond, aes(x = carat, y = price))
g <- g + xlab("Mass (carats)")
g <- g + ylab("Price (SIN $)")
g <- g + geom_point(size = 6, colour = "black", alpha = 0.2)
g <- g + geom_point(size = 5, colour = "blue", alpha = 0.2)
#black background and blue top colour
g <- g + geom_smooth(method = "lm", colour = "black")
#assumes y as outcome, x as predictor
g
```

The variation around the regression line is the residual variation.

The distances that make up the residual variation are called the residuals

notes on papersheet

```{r}
y <- diamond$price; x <- diamond$carat ; n <- length(y)
fit <- lm(y ~ x)
e <- resid(fit)
yhat <- predict(fit)
#ifyou don't give the prediction function no new data, it will predict at the observed x values.
max(abs(e - (y - yhat)))
max(abs(e - (y - coef(fit)[1] - coef(fit)[2]*x)))
```

```{r}
sum(e)
#close enough to zero
```

```{r}
sum(e * x)
#close enough to zero
```

```{r}
plot(diamond$carat, diamond$price,
     xlab = "Mass (carats)",
     ylab = "Price( SIN $",
     bg = "lightblue",
     col = "black", cex = 1.1, pch = 21, frame = FALSE)
abline(fit, lwd = 2)
for(i in 1 : n)
        lines(c(x[i], x[i]), c(y[i], yhat[i]), col = "red", lwd = 2)
```


my residuals are the red lines. the distances

if point is above the residual is positive, if below the residual is negative.

plotting residuals ~ mass

```{r}
plot(x, e,
     xlab = "Mass(carats)",
     ylab = "Residuals(SIN $)",
     bg = "lightblue",
     col = "black", cex =2, pch = 21, frame = F)
abline(h = 0, lwd = 2)
for (i in 1:n)
        lines(c(x[i], x[i]), c(e[i], 0), col = "red", lwd = 2)
```

from this plot we can see that there were a lot of diamonds with the same mass.



```{r}
x <- runif(100, -3, 3); y <- x + sin(x)+ rnorm(100, sd = .2);
g <- ggplot(data.frame(x = x, y = y), aes(x = x , y = y))
g <- g + geom_smooth(method = "lm", colour = "black")
g <- g + geom_point(size = 7, colour = "black", alpha = 0.4)
g <- g + geom_point(size = 5, colour = "red", alpha = 0.4)
g
```


this is not the correct model for this data.

there is a linear trend in the data, but the line is not accounting only for the non-linear trend in the data.

you can get meaningful information about trends from incorrect models. the goal is not finding the exact correct model in regression.

```{r}
g <- ggplot(data.frame(x =x , y = resid(lm(y ~ x))), aes(x = x, y = y))
g <- g + geom_hline(yintercept = 0, size = 2)
g <- g + geom_point(size = 7, colour = "black", alpha = 0.4)
g <- g + geom_point(size = 5, colour = "red", alpha = 0.4)
g <- g + xlab("X") + ylab("Residual")
g
```



now the sin trend is extremely apparent

```{r}
# Heteroskedasticity
x <- runif(100, 0 , 6); y <- x + rnorm(100, mean = 0, sd = 0.001*x);
g <- ggplot(data.frame(x = x, y = y), aes(x = x , y = y))
g <- g + geom_smooth(method= "lm", colour = "black")
g <- g + geom_point(size  =7, colour = "black", alpha = 0.4)
g <- g + geom_point(size = 5, colour = "red", alpha = 0.4)
g
```

difficult to see in the plot.

```{r}
#getting rid of the blank space can be helpful
g <- ggplot(data.frame(x = x, y = resid(lm(y ~ x))),
            aes(x = x, y = y))
g <- g + geom_hline(yintercept = 0, size  =2)
g <- g + geom_point(size  = 7, colour = "black", alpha = 0.4)
g <- g + geom_point(size =  5, colour = "red", alpha = 0.4)
g <- g + xlab("X") + ylab( "Residual")
g
```

here it is apparent.

heteroskedasticity is one of those things residuals plot is finding out

you couldn't see it on the regression plot.

```{r}
#let's add the residual plot of the diamond data
diamond$e <- resid(lm(price ~ carat , data = diamond))
g <- ggplot(diamond, aes(x = carat, y = e))
g <- g + xlab("Mass (carats)")
g <- g + ylab("Residual price (SIN$")
g <- g + geom_hline(yintercept = 0, size = 2)
g <- g + geom_point(size = 7, colour = "black", alpha = 0.5)
g <- g + geom_point(size = 5, colour = "blue", alpha = 0.2)
g
```

residuals have the same unit as the outcome

there doesn't appear a pattern. it's good. looks random.



```{r}
e <- c(resid(lm(price ~ 1, data = diamond)),
       resid(lm(price ~ carat, data = diamond)))
#first residual vector is the one where you just fit in an intercept
#residuals are the deviatios from the average price
#second one is the variation around the regression line.
fit <- factor(c(rep("Itc", nrow(diamond)),
                rep("Itc, slope", nrow(diamond))))
#factor variable that labels the set of residuals
g <- ggplot(data.frame(e = e, fit = fit), aes(y = e, x = fit, fill = fit))
g <- g + geom_dotplot(binaxis = "y", size = 2, stackdir = "center", binwidth =  20)
g<- g + xlab("Fitting approach")
g <- g + ylab("Residual price")
g

```


# Residual variance

```{r}
data(diamond)
y <- diamond$price; x<- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
summary(fit)$sigma
#residual st dev estimate
```

```{r}
sqrt(sum(resid(fit)^2)/(n - 2))
#confirmed with 46 degrees of freedom
```
R squared can be a misleading summary of model fit.


```{r}
data(anscombe)
example(anscombe)
```

same mean and variance of X and Y
identical correlations(hence same R^2)
same linear regression relationship.

BUT the models are significantly different.

1st plot - just a normal x and y relationship and regression.
2nd plot - there's clearly a missing term to explain the curvature
3rd plot - there's an outlier
4th plot - all data are at one value, and there is another value at the far end



# Inference in Regression

Inference is the process of drawing conclusions about a population using a sample. In statistical inference, we must account for the uncertainty in our estimates in a principled way. Hypothesis tests and confidence intervals are among the most common forms of statistical inference.

These statements apply generally, and, of course, to the regression setting that we've been studying. In the next few lectures, we'll cover inference in regression where we make some Gaussian assumptions about the errors.

```{r}
library(UsingR); data(diamond)
y <- diamond$price; x<- diamond$carat; n <- length(y)
beta1 <- cor(y,x)*sd(y)/sd(x)
#theoretical beta1
beta0 <- mean(y) - beta1 *mean(x)
#theoretical beta0
e <- y - beta0 - beta1*x
#residuals
sigma <- sqrt(sum(e^2)/(n-2))
#variability around the regression line
ssx <- sum((x - mean(x))^2)
#sums of squares of xs
seBeta0 <- (1/n + mean(x)^2 / ssx) ^ .5 * sigma
#standard error for Beta0
seBeta1 <- sigma/sqrt(ssx)
#standard error for Beta1
tBeta0 <- beta0 / seBeta0
#divided by its standard error
#t statistic that beta1 is zero or beta0 is zero
#we don't have to subtract the true value because the true value is accepted as zero under this hypothesis
#test of a 0 intercept. usually not very interesting
tBeta1 <- beta1 / seBeta1
#divided by its standard error
#t statistic that beta1 is zero or beta0 is zero
#we don't have to subtract the true value because the true value is accepted as zero under this hypothesis
#test of a 0 slope. this is the important part
pBeta0 <- 2 * pt(abs(tBeta0), df = n-2, lower.tail = FALSE)
#twice my t probability
pBeta1 <- 2 * pt(abs(tBeta1), df = n-2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std.Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
coefTable
```

Note : 
The t statistic is a measure of how extreme a statistical estimate is. You compute this statistic by subtracting the hypothesized value from the statistical estimate and then dividing by the estimated standard error. In many, but not all situation, the hypothesized value would be zero.

You have an indication that the hypothesized value is reasonable when the t-statistic is close to zero.  Alternately, you have an indication that the hypothesized value is not large enough when the t-statistic is large positive. Finally, you have an indication that the hypothesized value is too large when the t-statistic is large negative.

```{r}
coefTable
fit <- lm(y ~ x)
summary(fit)$coefficients
```

```{r}
summary(fit)
```

confidence intervals for our intercept and slope

```{r}
sumCoef <- summary(fit)$coefficients
sumCoef
```


confidence intervals
```{r}
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1,2]
#estimate +- .975th t quantile * st. error
sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2,2]
#estimate +- .975th t quantile * st. error

```

the slope is going to be interpreted in change in sNG dollar price per one mass increase in carat. But I might want it for a 0.1 increase. 1 carat increase is a big increase

```{r}
(sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1,2]) / 10
#estimate +- .975th t quantile * st. error / 10 (for slope * 0.1 units)
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2,2]) / 10
#estimate +- .975th t quantile * st. error / 10 (for slope * 0.1 units)
```

with 95% confidence we estimate that a 0.1 carat increase in diamond size is going to result in 355 to 388 increase in price in SNG $.

# Prediction

Prediction is a central concept for the data scientist. 
The entire course, Practical Machine Learning is on advanced prediction techniques.

```{r}
library(ggplot2)
newx <- data.frame(x = seq(min(x), max(x), length = 100))
p1 <- data.frame(predict(fit, newdata = newx, interval = ("confidence")))
#the predict function has the output of lm. some new x values(more dense). interval = ("confidence") means: "I want the interval around the estimated line at that particular value of x. not for a potential new y at that particular value of x.
# if you want the particular values of y around the estimated line, then you pick interval = ("prediction")
p2 <- data.frame(predict(fit, newdata = newx, interval = ("prediction")))
p1$interval = "confidence"
p2$interval = "prediction"
p1$x = newx$x
p2$x = newx$x
dat = rbind(p1, p2)
names(dat)[1] = "y"

g = ggplot(dat, aes(x = x, y = y))
g = g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2)
#ribbon has 2 parts. because it is going to be filled by the interval types. blue is the prediction interval
g = g + geom_line()
#the fitted line
g = g + geom_point(data = data.frame(x = x, y =y), aes(x = x, y = y), size = 4)
#observed data points are not in the data frame that I used to create the ggplot, so I have to give it a new data frame and a new aes
g
```


salmon is the confidence interval for the line. blue is the prediction interval. 

ozet = the leftover residual variation of my regression line is kinda the main source of the prediction interval being larger than the confidence interval. there's some uncertainty that doesn't go away with better estimation.

# Swirl

- Residual Variation
- Introduction to Multivariable Regression
- MultiVar Examples

## Residual Variation


