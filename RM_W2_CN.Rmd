---
title: "Regression Modelling Week 2"
author: "Yigit Ozan Berk"
date: "7/10/2019"
output: html_document
---

# Statistical Linear Regression Models

Up to this point, we’ve only considered estimation. Estimation is useful, but we also need to know how to extend our estimates to a population. This is the process of statistical inference. Our approach to statistical inference will be through a statistical model. At the bare minimum, we need a few distributional assumptions on the errors. However, we’ll focus on full model assumptions under Gaussianity.


notes on the paper sheet

## linear regression for prediction

example from 'diamond' data set from UsingR

```{r}
library(UsingR)
data(diamond)
library(ggplot2)
g <- ggplot(diamond, aes(x = carat, y = price))
g <- g + xlab("Mass (carats)")
g <- g + ylab("Price (SIN $)")
g <- g + geom_point(size = 6, colour = "black", alpha = 0.2)
g <- g + geom_point(size = 5, colour = "blue", alpha = 0.2)
#black background and blue top colour
g <- g + geom_smooth(method = "lm", colour = "black")
#assumes y as outcome, x as predictor
g

```

```{r}
fit <- lm(price ~ carat, data = diamond)
coef(fit)
```

we estimate an expected `r round(coef(fit)[2], 2)` (SIN) $ for every carat increase in mass of diamond.

```{r}
summary(fit)
```

```{r}
#mean centering
fit2 <- lm(price ~ I(carat - mean(carat)), data = diamond)
#if you want mathmatical functions within lm function, you have to surround them in the I() function
coef(fit2)
```

500 SIN $ is the average price for the average sized diamond which is about X = 0.2.

```{r}
#changing units of slope to 1/10 of a carat
fit3 <- lm(price ~ I(carat * 10), data = diamond)
coef(fit3)
```

```{r}
#predicting the price
newx <- c(0.16, 0.27, 0.34)
coef(fit)[1] + coef(fit)[2]*newx

```
```{r}
predict(fit, newdata = data.frame(carat = newx))
#takes output from several different prediction models
# if you don't use the newdata variable, it predicts yhat values of the original x values in the model 'fit'
```


# Residuals

Residuals represent variation left unexplained by our model. We emphasize the difference between residuals and errors. The errors unobservable true errors from the known coefficients, while residuals are the observable errors from the estimated coefficients. In a sense, the residuals are estimates of the errors.

Diamond data set from UsingR

```{r}
library(UsingR)
data(diamond)
library(ggplot2)

g <- ggplot(diamond, aes(x = carat, y = price))
g <- g + xlab("Mass (carats)")
g <- g + ylab("Price (SIN $)")
g <- g + geom_point(size = 6, colour = "black", alpha = 0.2)
g <- g + geom_point(size = 5, colour = "blue", alpha = 0.2)
#black background and blue top colour
g <- g + geom_smooth(method = "lm", colour = "black")
#assumes y as outcome, x as predictor
g
```

The variation around the regression line is the residual variation.

The distances that make up the residual variation are called the residuals

notes on papersheet

```{r}
y <- diamond$price; x <- diamond$carat ; n <- length(y)
fit <- lm(y ~ x)
e <- resid(fit)
yhat <- predict(fit)
#ifyou don't give the prediction function no new data, it will predict at the observed x values.
max(abs(e - (y - yhat)))
max(abs(e - (y - coef(fit)[1] - coef(fit)[2]*x)))
```

```{r}
sum(e)
#close enough to zero
```

```{r}
sum(e * x)
#close enough to zero
```

```{r}
plot(diamond$carat, diamond$price,
     xlab = "Mass (carats)",
     ylab = "Price( SIN $",
     bg = "lightblue",
     col = "black", cex = 1.1, pch = 21, frame = FALSE)
abline(fit, lwd = 2)
for(i in 1 : n)
        lines(c(x[i], x[i]), c(y[i], yhat[i]), col = "red", lwd = 2)
```


my residuals are the red lines. the distances

if point is above the residual is positive, if below the residual is negative.

plotting residuals ~ mass

```{r}
plot(x, e,
     xlab = "Mass(carats)",
     ylab = "Residuals(SIN $)",
     bg = "lightblue",
     col = "black", cex =2, pch = 21, frame = F)
abline(h = 0, lwd = 2)
for (i in 1:n)
        lines(c(x[i], x[i]), c(e[i], 0), col = "red", lwd = 2)
```

from this plot we can see that there were a lot of diamonds with the same mass.



```{r}
x <- runif(100, -3, 3); y <- x + sin(x)+ rnorm(100, sd = .2);
g <- ggplot(data.frame(x = x, y = y), aes(x = x , y = y))
g <- g + geom_smooth(method = "lm", colour = "black")
g <- g + geom_point(size = 7, colour = "black", alpha = 0.4)
g <- g + geom_point(size = 5, colour = "red", alpha = 0.4)
g
```


this is not the correct model for this data.

there is a linear trend in the data, but the line is not accounting only for the non-linear trend in the data.

you can get meaningful information about trends from incorrect models. the goal is not finding the exact correct model in regression.

```{r}
g <- ggplot(data.frame(x =x , y = resid(lm(y ~ x))), aes(x = x, y = y))
g <- g + geom_hline(yintercept = 0, size = 2)
g <- g + geom_point(size = 7, colour = "black", alpha = 0.4)
g <- g + geom_point(size = 5, colour = "red", alpha = 0.4)
g <- g + xlab("X") + ylab("Residual")
g
```



now the sin trend is extremely apparent

```{r}
# Heteroskedasticity
x <- runif(100, 0 , 6); y <- x + rnorm(100, mean = 0, sd = 0.001*x);
g <- ggplot(data.frame(x = x, y = y), aes(x = x , y = y))
g <- g + geom_smooth(method= "lm", colour = "black")
g <- g + geom_point(size  =7, colour = "black", alpha = 0.4)
g <- g + geom_point(size = 5, colour = "red", alpha = 0.4)
g
```

difficult to see in the plot.

```{r}
#getting rid of the blank space can be helpful
g <- ggplot(data.frame(x = x, y = resid(lm(y ~ x))),
            aes(x = x, y = y))
g <- g + geom_hline(yintercept = 0, size  =2)
g <- g + geom_point(size  = 7, colour = "black", alpha = 0.4)
g <- g + geom_point(size =  5, colour = "red", alpha = 0.4)
g <- g + xlab("X") + ylab( "Residual")
g
```

here it is apparent.

heteroskedasticity is one of those things residuals plot is finding out

you couldn't see it on the regression plot.

```{r}
#let's add the residual plot of the diamond data
diamond$e <- resid(lm(price ~ carat , data = diamond))
g <- ggplot(diamond, aes(x = carat, y = e))
g <- g + xlab("Mass (carats)")
g <- g + ylab("Residual price (SIN$")
g <- g + geom_hline(yintercept = 0, size = 2)
g <- g + geom_point(size = 7, colour = "black", alpha = 0.5)
g <- g + geom_point(size = 5, colour = "blue", alpha = 0.2)
g
```

residuals have the same unit as the outcome

there doesn't appear a pattern. it's good. looks random.



```{r}
e <- c(resid(lm(price ~ 1, data = diamond)),
       resid(lm(price ~ carat, data = diamond)))
#first residual vector is the one where you just fit in an intercept
#residuals are the deviatios from the average price
#second one is the variation around the regression line.
fit <- factor(c(rep("Itc", nrow(diamond)),
                rep("Itc, slope", nrow(diamond))))
#factor variable that labels the set of residuals
g <- ggplot(data.frame(e = e, fit = fit), aes(y = e, x = fit, fill = fit))
g <- g + geom_dotplot(binaxis = "y", size = 2, stackdir = "center", binwidth =  20)
g<- g + xlab("Fitting approach")
g <- g + ylab("Residual price")
g

```


# Residual variance

```{r}
data(diamond)
y <- diamond$price; x<- diamond$carat; n <- length(y)
fit <- lm(y ~ x)
summary(fit)$sigma
#residual st dev estimate
```

```{r}
sqrt(sum(resid(fit)^2)/(n - 2))
#confirmed with 46 degrees of freedom
```
R squared can be a misleading summary of model fit.


```{r}
data(anscombe)
example(anscombe)
```

same mean and variance of X and Y
identical correlations(hence same R^2)
same linear regression relationship.

BUT the models are significantly different.

1st plot - just a normal x and y relationship and regression.
2nd plot - there's clearly a missing term to explain the curvature
3rd plot - there's an outlier
4th plot - all data are at one value, and there is another value at the far end



# Inference in Regression

Inference is the process of drawing conclusions about a population using a sample. In statistical inference, we must account for the uncertainty in our estimates in a principled way. Hypothesis tests and confidence intervals are among the most common forms of statistical inference.

These statements apply generally, and, of course, to the regression setting that we've been studying. In the next few lectures, we'll cover inference in regression where we make some Gaussian assumptions about the errors.

```{r}
library(UsingR); data(diamond)
y <- diamond$price; x<- diamond$carat; n <- length(y)
beta1 <- cor(y,x)*sd(y)/sd(x)
#theoretical beta1
beta0 <- mean(y) - beta1 *mean(x)
#theoretical beta0
e <- y - beta0 - beta1*x
#residuals
sigma <- sqrt(sum(e^2)/(n-2))
#variability around the regression line
ssx <- sum((x - mean(x))^2)
#sums of squares of xs
seBeta0 <- (1/n + mean(x)^2 / ssx) ^ .5 * sigma
#standard error for Beta0
seBeta1 <- sigma/sqrt(ssx)
#standard error for Beta1
tBeta0 <- beta0 / seBeta0
#divided by its standard error
#t statistic that beta1 is zero or beta0 is zero
#we don't have to subtract the true value because the true value is accepted as zero under this hypothesis
#test of a 0 intercept. usually not very interesting
tBeta1 <- beta1 / seBeta1
#divided by its standard error
#t statistic that beta1 is zero or beta0 is zero
#we don't have to subtract the true value because the true value is accepted as zero under this hypothesis
#test of a 0 slope. this is the important part
pBeta0 <- 2 * pt(abs(tBeta0), df = n-2, lower.tail = FALSE)
#twice my t probability
pBeta1 <- 2 * pt(abs(tBeta1), df = n-2, lower.tail = FALSE)
coefTable <- rbind(c(beta0, seBeta0, tBeta0, pBeta0), c(beta1, seBeta1, tBeta1, pBeta1))
colnames(coefTable) <- c("Estimate", "Std.Error", "t value", "P(>|t|)")
rownames(coefTable) <- c("(Intercept)", "x")
coefTable
```

Note : 
The t statistic is a measure of how extreme a statistical estimate is. You compute this statistic by subtracting the hypothesized value from the statistical estimate and then dividing by the estimated standard error. In many, but not all situation, the hypothesized value would be zero.

You have an indication that the hypothesized value is reasonable when the t-statistic is close to zero.  Alternately, you have an indication that the hypothesized value is not large enough when the t-statistic is large positive. Finally, you have an indication that the hypothesized value is too large when the t-statistic is large negative.

```{r}
coefTable
fit <- lm(y ~ x)
summary(fit)$coefficients
```

```{r}
summary(fit)
```

confidence intervals for our intercept and slope

```{r}
sumCoef <- summary(fit)$coefficients
sumCoef
```


confidence intervals
```{r}
sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1,2]
#estimate +- .975th t quantile * st. error
sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2,2]
#estimate +- .975th t quantile * st. error

```

the slope is going to be interpreted in change in sNG dollar price per one mass increase in carat. But I might want it for a 0.1 increase. 1 carat increase is a big increase

```{r}
(sumCoef[1,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[1,2]) / 10
#estimate +- .975th t quantile * st. error / 10 (for slope * 0.1 units)
(sumCoef[2,1] + c(-1, 1) * qt(.975, df = fit$df) * sumCoef[2,2]) / 10
#estimate +- .975th t quantile * st. error / 10 (for slope * 0.1 units)
```

with 95% confidence we estimate that a 0.1 carat increase in diamond size is going to result in 355 to 388 increase in price in SNG $.

# Prediction

Prediction is a central concept for the data scientist. 
The entire course, Practical Machine Learning is on advanced prediction techniques.

```{r}
library(ggplot2)
newx <- data.frame(x = seq(min(x), max(x), length = 100))
p1 <- data.frame(predict(fit, newdata = newx, interval = ("confidence")))
#the predict function has the output of lm. some new x values(more dense). interval = ("confidence") means: "I want the interval around the estimated line at that particular value of x. not for a potential new y at that particular value of x.
# if you want the particular values of y around the estimated line, then you pick interval = ("prediction")
p2 <- data.frame(predict(fit, newdata = newx, interval = ("prediction")))
p1$interval = "confidence"
p2$interval = "prediction"
p1$x = newx$x
p2$x = newx$x
dat = rbind(p1, p2)
names(dat)[1] = "y"

g = ggplot(dat, aes(x = x, y = y))
g = g + geom_ribbon(aes(ymin = lwr, ymax = upr, fill = interval), alpha = 0.2)
#ribbon has 2 parts. because it is going to be filled by the interval types. blue is the prediction interval
g = g + geom_line()
#the fitted line
g = g + geom_point(data = data.frame(x = x, y =y), aes(x = x, y = y), size = 4)
#observed data points are not in the data frame that I used to create the ggplot, so I have to give it a new data frame and a new aes
g
```


salmon is the confidence interval for the line. blue is the prediction interval. 

ozet = the leftover residual variation of my regression line is kinda the main source of the prediction interval being larger than the confidence interval. there's some uncertainty that doesn't go away with better estimation.

# Swirl

- Residual Variation
- Introduction to Multivariable Regression
- MultiVar Examples

## Residual Variation

As shown in the slides, residuals are useful for indicating how well data
| points fit a statistical model. They "can be thought of as the outcome (Y)
| with the linear association of the predictor (X) removed. One differentiates
| residual variation (variation after removing the predictor) from systematic
| variation (variation explained by the regression model)."

...

  |======                                                                 |   9%
| It can also be shown that, given a model, the maximum likelihood estimate of
| the variance of the random error is the average squared residual. However,
| since our linear model with one predictor requires two parameters we have only
| (n-2) degrees of freedom. Therefore, to calculate an "average" squared
| residual to estimate the variance we use the formula 1/(n-2) * (the sum of the
| squared residuals). If we divided the sum of the squared residuals by n,
| instead of n-2, the result would give a biased estimate.


First, we'll use the residuals (fit$residuals) of our model to estimate the
| standard deviation (sigma) of the error. We've already defined n for you as
| the number of points in Galton's dataset (928).

 First, we'll use the residuals (fit$residuals) of our model to estimate the
| standard deviation (sigma) of the error. We've already defined n for you as
| the number of points in Galton's dataset (928).

...

  |================                                                       |  23%
| Calculate the sum of the squared residuals divided by the quantity (n-2).
| Then take the square root.

> sqrt(sum(fit$residuals^2)/(n-2))
[1] 2.238547

| That's correct!

  |===================                                                    |  27%
| Now look at the "sigma" portion of the summary of fit, "summary(fit)$sigma".

> summary(fit)$sigma
[1] 2.238547

| That's the answer I was looking for.
Another cool thing - take the sqrt of "deviance(fit)/(n-2)" at the R prompt.

> deviance(fit)/(n-2)
[1] 5.011094

| That's not the answer I was looking for, but try again. Or, type info() for
| more options.

| Type "sqrt(deviance(fit)/(n-2))" at the R prompt.

> sqrt(deviance(fit)/(n-2))
[1] 2.238547

| That's correct!

Another useful fact shown in the slides was

...

  |================================                                       |  45%
| Total Variation = Residual Variation + Regression Variation

Recall the beauty of the slide full of algebra which proved this fact. It had
| a bunch of Y's, some with hats and some with bars and several summations of
| squared values. The Y's with hats were the estimates provided by the model.
| (They were on the regression line.) The Y with the bar was the mean or average
| of the data. Which sum of squared term represented Total Variation?

1: Yi_hat-mean(Yi)
2: Yi-mean(Yi)
3: Yi-Yi_hat

Selection: 2

| Keep up the great work!

  |=======================================                                |  55%
| Which sum of squared term represents Residual Variation?

1: Yi_hat-mean(Yi)
2: Yi-Yi_hat
3: Yi-mean(Yi)

Selection: 2

| Excellent job!

  |==========================================                             |  59%
| The term R^2 represents the percent of total variation described by the model,
| the regression variation (the term we didn't ask about in the preceding
| multiple choice questions). Also, since it is a percent we need a ratio or
| fraction of sums of squares. Let's do this now for our Galton data.


Recall that centering data means subtracting the mean from each data point.
| Now calculate the sum of the squares of the centered children's heights and
| store the result in a variable called sTot. This represents the Total
| Variation of the data.

> sTot <- sum((galton$child - mean(galton$child))^2)

| That's not the answer I was looking for, but try again. Or, type info() for
| more options.

| Type "sTot <- sum((galton$child-mu)^2)" at the R prompt.

> sTot <- sum((galton$child - mu)^2)

| You nailed it! Good job!

  |====================================================                   |  73%
| Now create the variable sRes. Use the R function deviance to calculate the sum
| of the squares of the residuals. These are the distances between the
| children's heights and the regression line. This represents the Residual
| Variation.


Type "sRes <- deviance(fit)" at the R prompt.

> sRes <- deviance(fit)

| You got it right!

  |=======================================================                |  77%
| Finally, the ratio sRes/sTot represents the percent of total variation
| contributed by the residuals. To find the percent contributed by the model,
| i.e., the regression variation, subtract the fraction sRes/sTot from 1.  This
| is the value R^2.


Finally, the ratio sRes/sTot represents the percent of total variation
| contributed by the residuals. To find the percent contributed by the model,
| i.e., the regression variation, subtract the fraction sRes/sTot from 1.  This
| is the value R^2.

> 1 - sRes/sTot
[1] 0.2104629

| You are doing so well!

  |==========================================================             |  82%
| For fun you can compare your result to the values shown in
| summary(fit)$r.squared to see if it looks familiar. Do this now.

summary(fit)$r.squared
[1] 0.2104629

| That's correct!

  |=============================================================          |  86%
| To see some real magic, compute the square of the correlation of the galton
| data, the children and parents. Use the R function cor.



To see some real magic, compute the square of the correlation of the galton
| data, the children and parents. Use the R function cor.

> cor(galton$child, galton$parent)^2
[1] 0.2104629

| Perseverance, that's the answer.

  |=================================================================      |  91%
| We'll now summarize useful facts about R^2. It is the percentage of variation
| explained by the regression model. As a percentage it is between 0 and 1. It
| also equals the sample correlation squared. However, R^2 doesn't tell the
| whole story.



# Introduction to Multivariable Regression

In this lesson we'll illustrate that regression in many variables amounts to a
| series of regressions in one. Using regression in one variable, we'll show how
| to eliminate any chosen regressor, thus reducing a regression in N variables,
| to a regression in N-1. Hence, if we know how to do a regression in 1 variable,
| we can do a regression in 2. Once we know how to do a regression in 2
| variables, we can do a regression in 3, and so on. We begin with the galton
| data and a review of eliminating the intercept by subtracting the means.

| When we perform a regression in one variable, such as lm(child ~ parent,
| galton), we get two coefficients, a slope and an intercept. The intercept is
| really the coefficient of a special regressor which has the same value, 1, at
| every sample. The function, lm, includes this regressor by default.

We'll demonstrate by substituting an all-ones regressor of our own. This
| regressor must have the same number of samples as galton (928.) Create such an
| object and name it ones, using ones <- rep(1, nrow(galton)), or some equivalent
| expression.

> ones <- rep(1, nrow(galton))

| That's the answer I was looking for.

  |===========                                                             |  15%
| The galton data has already been loaded. The default intercept can be excluded
| by using -1 in the formula. Perform a regression which substitutes our
| regressor, ones, for the default using lm(child ~ ones + parent -1, galton).
| Since we want the result to print, don't assign it to a variable.



lm(child~ ones + parent - 1, galton)

Call:
lm(formula = child ~ ones + parent - 1, data = galton)

Coefficients:
   ones   parent  
23.9415   0.6463  


| You nailed it! Good job!

  |==============                                                          |  19%
| The coefficient of ones is 23.9415. Now use the default, lm(child ~ parent,
| galton), to show the intercept has the same value. This time, DO NOT suppress
| the intercept with -1.

> lm(child ~ parent, galton)

Call:
lm(formula = child ~ parent, data = galton)

Coefficients:
(Intercept)       parent  
    23.9415       0.6463  


| That's a job well done!

  |=================                                                       |  23%
| The regression in one variable given by lm(child ~ parent, galton) really
| involves two regressors, the variable, parent, and a regressor of all ones.



1: False
2: True

Selection: 2

| Keep working like that and you'll get there!

  |===================                                                     |  27%
| In earlier lessons we demonstrated that the regression line given by lm(child ~
| parent, galton) goes through the point x=mean(parent), y=mean(child). We also
| showed that if we subtract the mean from each variable, the regression line
| goes through the origin, x=0, y=0, hence its intercept is zero. Thus, by
| subtracting the means, we eliminate one of the two regressors, the constant,
| leaving just one, parent. The coefficient of the remaining regressor is the
| slope.

Subtracting the means to eliminate the intercept is a special case of a general
| technique which is sometimes called Gaussian Elimination. As it applies here,
| the general technique is to pick one regressor and to replace all other
| variables by the residuals of their regressions against that one.

Suppose, as claimed, that subtracting a variable's mean is a special case of
| replacing the variable with a residual. In this special case, it would be the
| residual of a regression against what?

1: The variable itself
2: The constant, 1
3: The outcome

Selection: 2

| That's correct!

  |============================                                            |  38%
| The mean of a variable is the coefficient of its regression against the
| constant, 1. Thus, subtracting the mean is equivalent to replacing a variable
| by the residual of its regression against 1. In an R formula, the constant
| regressor can be represented by a 1 on the right hand side. Thus, the
| expression, lm(child ~ 1, galton), regresses child against the constant, 1.
| Recall that in the galton data, the mean height of a child was 68.09 inches.
| Use lm(child ~ 1, galton) to compare the resulting coefficient (the intercept)
| and the mean height of 68.09. Since we want the result to print, don't assign
| it a name.


lm(child ~ 1, galton)

Call:
lm(formula = child ~ 1, data = galton)

Coefficients:
(Intercept)  
      68.09  


| That's the answer I was looking for.

  |==============================                                          |  42%
| The mean of a variable is equal to its regression against the constant, 1.

1: False
2: True

Selection: 2

| Keep up the great work!

  |=================================                                       |  46%
| To illustrate the general case we'll use the trees data from the datasets
| package. The idea is to predict the Volume of timber which a tree might produce
| from measurements of its Height and Girth. To avoid treating the intercept as a
| special case, we have added a column of 1's to the data which we shall use in
| its place. Please take a moment to inspect the data using either View(trees) or
| head(trees).

head(trees)
  Constant Girth Height Volume
1        1   8.3     70   10.3
2        1   8.6     65   10.3
3        1   8.8     63   10.2
4        1  10.5     72   16.4
5        1  10.7     81   18.8
6        1  10.8     83   19.7

| All that practice is paying off!

  |====================================                                    |  50%
| A file of relevant code has been copied to your working directory and sourced.
| The file, elimination.R, should have appeared in your editor. If not, please
| open it manually.

The general technique is to pick one predictor and to replace all other
| variables by the residuals of their regressions against that one. The function,
| regressOneOnOne, in eliminate.R performs the first step of this process. Given
| the name of a predictor and one other variable, other, it returns the residual
| of other when regressed against predictor. In its first line, labeled Point A,
| it creates a formula. Suppose that predictor were 'Girth' and other were
| 'Volume'. What formula would it create?

1: Volume ~ Girth - 1
2: Girth ~ Volume - 1
3: Volume ~ Girth

Selection: 1

| Excellent job!

  |==========================================                              |  58%
| The remaining function, eliminate, applies regressOneOnOne to all variables
| except a given predictor and collects the residuals in a data frame. We'll
| first show that when we eliminate one regressor from the data, a regression on
| the remaining will produce their correct coefficients. (Of course, the
| coefficient of the eliminated regressor will be missing, but more about that
| later.)


```{r}
# Regress the given variable on the given predictor,
# suppressing the intercept, and return the residual.
regressOneOnOne <- function(predictor, other, dataframe){
  # Point A. Create a formula such as Girth ~ Height -1
  formula <- paste0(other, " ~ ", predictor, " - 1")
  # Use the formula in a regression and return the residual.
  resid(lm(formula, dataframe))
}

# Eliminate the specified predictor from the dataframe by
# regressing all other variables on that predictor
# and returning a data frame containing the residuals
# of those regressions.
eliminate <- function(predictor, dataframe){
  # Find the names of all columns except the predictor.
  others <- setdiff(names(dataframe), predictor)
  # Calculate the residuals of each when regressed against the given predictor
  temp <- sapply(others, function(other)regressOneOnOne(predictor, other, dataframe))
  # sapply returns a matrix of residuals; convert to a data frame and return.
  as.data.frame(temp)
}
```



For reference, create a model named fit, based on all three regressors, Girth,
| Height, and Constant, and assign the result to a variable named fit. Use an
| expression such as fit <- lm(Volume ~ Girth + Height + Constant -1, trees).
| Don't forget the -1, and be sure to name the model fit for later use.

fit <- lm(Volume ~ Girth + Height + Constant -1, trees)

| Nice work!

  |===============================================                         |  65%
| Now let's eliminate Girth from the data set. Call the reduced data set trees2
| to indicate it has only 2 regressors. Use the expression trees2 <-
| eliminate("Girth", trees)

 trees2 <- eliminate("Girth", trees)

| That's a job well done!

  |==================================================                      |  69%
| Use head(trees2) or View(trees2) to inspect the reduced data set.

> head(trees2)
   Constant   Height     Volume
1 0.4057735 24.38809  -9.793826
2 0.3842954 17.73947 -10.520109
3 0.3699767 14.64038 -11.104298
4 0.2482677 14.29818  -9.019900
5 0.2339490 22.19910  -7.104089
6 0.2267896 23.64956  -6.446183

| Keep up the great work!

  |=====================================================                   |  73%
| Why, in trees2, is the Constant column not constant?

1: There must be some mistake
2: Computational precision was insufficient.
3: The constant, 1, has been replaced by its residual when regressed against Girth.

3

| Your dedication is inspiring!

  |=======================================================                 |  77%
| Now create a model, called fit2, using the reduced data set. Use an expression
| such as fit2 <- lm(Volume ~ Height + Constant -1, trees2). Don't forget to use
| -1 in the formula.

fit2 <- lm(Volume ~ Height + Constant - 1, trees2)

| You got it!

  |==========================================================              |  81%
| Use the expression lapply(list(fit, fit2), coef) to print coefficients of fit
| and fit2 for comparison.

> lapply(list(fit, fit2), coef)
[[1]]
      Girth      Height    Constant 
  4.7081605   0.3392512 -57.9876589 

[[2]]
     Height    Constant 
  0.3392512 -57.9876589 


| Excellent work!

  |=============================================================           |  85%
| The coefficient of the eliminated variable is missing, of course. One way to
| get it would be to go back to the original data, trees, eliminate a different
| regressor, such as Height, and do another 2 variable regession, as above. There
| are much more efficient ways, but efficiency is not the point of this
| demonstration. We have shown how to reduce a regression in 3 variables to a
| regression in 2. We can go further and eliminate another variable, reducing a
| regression in 2 variables to a regression in 1.

Here is the final step. We have used eliminate("Height", trees2) to reduce the
| data to the outcome, Volume, and the Constant regressor. We have regressed
| Volume on Constant, and printed the coefficient as shown in the command above
| the answer. As you can see, the coefficient of Constant agrees with previous
| values.


Call:
lm(formula = Volume ~ Constant - 1, data = eliminate("Height", 
    trees2))

Coefficients:
Constant  
  -57.99  
  
  Suppose we were given a multivariable regression problem involving an outcome
| and N regressors, where N > 1. Using only single-variable regression, how can
| the problem be reduced to a problem with only N-1 regressors?

1: Pick any regressor and replace the outcome and all other regressors by their residuals against the chosen one.
2: Subtract the mean from the outcome and each regressor.

Selection: 1

| Excellent job!

  |=====================================================================   |  96%
| We have illustrated that regression in many variables amounts to a series of
| regressions in one. The actual algorithms used by functions such as lm are more
| efficient, but are computationally equivalent to what we have done. That is,
| the algorithms use equivalent steps but combine them more efficiently and
| abstractly. This completes the lesson.


# MultiVar Examples

```{r}
makelms <- function(){
  # Store the coefficient of linear models with different independent variables
  cf <- c(coef(lm(Fertility ~ Agriculture, swiss))[2], 
          coef(lm(Fertility ~ Agriculture + Catholic,swiss))[2],
          coef(lm(Fertility ~ Agriculture + Catholic + Education,swiss))[2],
          coef(lm(Fertility ~ Agriculture + Catholic + Education + Examination,swiss))[2],
          coef(lm(Fertility ~ Agriculture + Catholic + Education + Examination +Infant.Mortality, swiss))[2])
  print(cf)
}

# Regressor generation process 1.
rgp1 <- function(){
  print("Processing. Please wait.")
  # number of samples per simulation
  n <- 100
  # number of simulations
  nosim <- 1000
  # set seed for reproducability
  set.seed(4321)
  # Point A:
  x1 <- rnorm(n)
  x2 <- rnorm(n)
  x3 <- rnorm(n)
  # Point B:
  betas <- sapply(1 : nosim, function(i)makelms(x1, x2, x3))
  round(apply(betas, 1, var), 5)
}

# Regressor generation process 2.
rgp2 <- function(){
  print("Processing. Please wait.")
  # number of samples per simulation
  n <- 100
  # number of simulations
  nosim <- 1000
  # set seed for reproducability
  set.seed(4321)
  # Point C:
  x1 <- rnorm(n)
  x2 <- x1/sqrt(2) + rnorm(n) /sqrt(2)
  x3 <- x1 * 0.95 + rnorm(n) * sqrt(1 - 0.95^2)
  # Point D:
  betas <- sapply(1 : nosim, function(i)makelms(x1, x2, x3))
  round(apply(betas, 1, var), 5)
}

```

 In this lesson, we'll look at some examples of regression models with more than
| one variable. We'll begin with the Swiss data which we've taken the liberty to
| load for you. This data is part of R's datasets package. It was gathered in
| 1888, a time of demographic change in Switzerland, and measured six quantities
| in 47 French-speaking provinces of Switzerland. We used the code from the
| slides (the R function pairs) to display here a 6 by 6 array of scatterplots
| showing pairwise relationships between the variables. All of the variables,
| except for fertility, are proportions of population. For example, "Examination"
| shows the percentage of draftees receiving the highest mark on an army exam,
| and "Education" the percentage of draftees with education beyond primary
| school.

First, use the R function lm to generate the linear model "all" in which
| Fertility is the variable dependent on all the others. Use the R shorthand "."
| to represent the five independent variables in the formula passed to lm.
| Remember the data is "swiss".


all <- lm(Fertility ~ ., swiss)

| Great job!

  |=============                                                           |  17%
| Now look at the summary of the linear model all.



summary(all)

Call:
lm(formula = Fertility ~ ., data = swiss)

Residuals:
     Min       1Q   Median       3Q      Max 
-15.2743  -5.2617   0.5032   4.1198  15.3213 

Coefficients:
                 Estimate Std. Error t value Pr(>|t|)    
(Intercept)      66.91518   10.70604   6.250 1.91e-07 ***
Agriculture      -0.17211    0.07030  -2.448  0.01873 *  
Examination      -0.25801    0.25388  -1.016  0.31546    
Education        -0.87094    0.18303  -4.758 2.43e-05 ***
Catholic          0.10412    0.03526   2.953  0.00519 ** 
Infant.Mortality  1.07705    0.38172   2.822  0.00734 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 7.165 on 41 degrees of freedom
Multiple R-squared:  0.7067,	Adjusted R-squared:  0.671 
F-statistic: 19.76 on 5 and 41 DF,  p-value: 5.594e-10


| That's a job well done!

  |================                                                        |  22%
| Recall that the Estimates are the coefficients of the independent variables of
| the linear model (all of which are percentages) and they reflect an estimated
| change in the dependent variable (fertility) when the corresponding independent
| variable changes. So, for every 1% increase in percent of males involved in
| agriculture as an occupation we expect a .17 decrease in fertility, holding all
| the other variables constant; for every 1% increase in Catholicism, we expect a
| .10 increase in fertility, holding all other variables constant.

The "*" at the far end of the row indicates that the influence of Agriculture
| on Fertility is significant. At what alpha level is the t-test of Agriculture
| significant?

1: R doesn't say
2: 0.01
3: 0.05
4: 0.1

Selection: 2

| Try again. Getting it right on the first try is boring anyway!

| Look at the "Signif. codes" line in the summary output.

1: 0.05
2: 0.1
3: R doesn't say
4: 0.01

Selection: 4

| You almost had it, but not quite. Try again.

| Look at the "Signif. codes" line in the summary output.

1: R doesn't say
2: 0.01
3: 0.1
4: 0.05

Selection: 4

| You got it right!

  |======================                                                  |  30%
| Now generate the summary of another linear model (don't store it in a new
| variable) in which Fertility depends only on agriculture.

summary(lm(Fertility ~ Agriculture, swiss))

Call:
lm(formula = Fertility ~ Agriculture, data = swiss)

Residuals:
     Min       1Q   Median       3Q      Max 
-25.5374  -7.8685  -0.6362   9.0464  24.4858 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept) 60.30438    4.25126  14.185   <2e-16 ***
Agriculture  0.19420    0.07671   2.532   0.0149 *  
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 11.82 on 45 degrees of freedom
Multiple R-squared:  0.1247,	Adjusted R-squared:  0.1052 
F-statistic: 6.409 on 1 and 45 DF,  p-value: 0.01492


| You are quite good my friend!

  |=========================                                               |  35%
| What is the coefficient of agriculture in this new model?

1: 0.19420
2: 0.07671
3: 60.30438
4: *

| What is the coefficient of agriculture in this new model?

1: 0.19420
2: 0.07671
3: 60.30438
4: *

Selection: 2

| Not quite! Try again.

| Look at the "Estimate" column and "Agriculture" row of the summary data you
| just generated.

1: 60.30438
2: *
3: 0.07671
4: 0.19420

Selection: 4

| You are amazing!

  |============================                                            |  39%
| The interesting point is that the sign of the Agriculture coefficient changed
| from negative (when all the variables were included in the model) to positive
| (when the model only considered Agriculture). Obviously the presence of the
| other factors affects the influence Agriculture has on Fertility.

Let's consider the relationship between some of the factors. How would you
| expect level Education and performance on an Examination to be related?

1: I would not be able to guess without more information
2: They would be correlated
3: They would be uncorrelated

Selection: 2

| Excellent job!

  |==================================                                      |  48%
| Now check your intuition with the R command "cor". This computes the
| correlation between Examination and Education.

> cor(swiss$Education, swiss$Examination)
[1] 0.6984153

| You are amazing!

  |======================================                                  |  52%
| The correlation of .6984 shows the two are correlated. Now find the correlation
| between Agriculture and Education.

cor(swiss$Education, swiss$Agriculture)
[1] -0.6395225

| That's a job well done!

  |=========================================                               |  57%
| The negative correlation (-.6395) between Agriculture and Education might be
| affecting Agriculture's influence on Fertility. I've loaded and sourced the
| file swissLMs.R in your working directory. In it is a function makelms() which
| generates a sequence of five linear models. Each model has one more independent
| variable than the preceding model, so the first has just one independent
| variable, Agriculture, and the last has all 5. I've tried loading the source
| code in your editor. If I haven't done this, open the file manually so you can
| look at the code.

Now run the function makelms() to see how the addition of variables affects the
| coefficient of Agriculture in the models.

> makelms()
Agriculture Agriculture Agriculture Agriculture Agriculture 
  0.1942017   0.1095281  -0.2030377  -0.2206455  -0.1721140 

| You are doing so well!

  |===============================================                         |  65%
| The addition of which variable changes the sign of Agriculture's coefficient
| from positive to negative?

1: Infant.Mortality
2: Catholic
3: Examination
4: Education

4

| Your dedication is inspiring!

  |==================================================                      |  70%
| Now we'll show what happens when we add a variable that provides no new linear
| information to a model. Create a variable ec that is the sum of
| swiss$Examination and swiss$Catholic.


ec <- swiss$Examination + swiss$Catholic

| You are amazing!

  |=====================================================                   |  74%
| Now generate a new model efit with Fertility as the dependent variable and the
| remaining 5 of the original variables AND ec as the independent variables. Use
| the R shorthand ". + ec" for the righthand side of the formula.

efit <- lm(Fertility ~ . + ec, swiss)

| Nice work!

  |========================================================                |  78%
| We'll see that R ignores this new term since it doesn't add any information to
| the model.

Subtract the efit coefficients from the coefficients of the first model you
| created, all.

> all$coefficients - efit$coefficients
     (Intercept)      Agriculture      Examination        Education 
               0                0                0                0 
        Catholic Infant.Mortality               ec 
               0                0               NA 
Warning message:
In all$coefficients - efit$coefficients :
  longer object length is not a multiple of shorter object length

| You are really on a roll!

  |===============================================================         |  87%
| Which is the coefficient of ec?

1: NA
2: 0
3: I haven't a clue.

Selection: 1

| Keep up the great work!

  |==================================================================      |  91%
| This tells us that

1: Adding ec doesn't change the model
2: Adding ec zeroes out the coefficients
3: R is really cool


 1

| Keep up the great work!

  |=====================================================================   |  96%
| Congrats! You've concluded this first lesson on multivariable linear models.













# book examples
Test the hypothesis of no linear relationship between hp and mpg chapter5-6

```{r}
summary(lm(mpg ~ hp, data = mtcars))$coef

```

p value of the slope is 1 * 10^(-7). this is smaller than alpha = 0.05. Therefore we reject the null hypothesis that there is no relationship between hp and mpg(or that the slope is equal to zero).

