---
title: "week 4"
author: "Yigit Ozan Berk"
date: "8/1/2019"
output: html_document
---

deviance = residual sum of squares

using F statistic when checking anova for linear models, using chisquared statistic when checking generalized linear models(count data and binary data)



# topics

- GLMs
- Logistic Regression
- Poisson Regression
- Hodgepodge
- Swirl Exc.
- Quiz
- Course Project

# GLMs

Generalized linear models (GLMs) were a great advance in statistical modeling. The original manuscript with the GLM framework was from Nelder and Wedderburn in 1972. in the Journal of the Royal Statistical Society. The McCullagh and Nelder book1 is the famous standard treatise on the subject.

Recall linear models. Linear models are the most useful applied statistical technique. However, they are not without their limitations. Additive response models don’t make much sense if the response is discrete, or strictly positive. Additive error models often don’t make sense, for example, if the outcome has to be positive. Transformations, such as taking a cube root of a count outcome, are often hard to interpret.In addition, there’s value in modeling the data on the scale that it was collected. Particularly interpretable transformations, natural logarithms in specific, aren’t applicable for negative or zero values.

The generalized linear model is family of models that includes linear models. By extending the family, it handles many of the issues with linear models, but at the expense of some complexity and loss of some of the mathematical tidiness. A GLM involves three components

An exponential family model for the response.
A systematic component via a linear predictor.
A link function that connects the means of the response to the linear predictor.
The three most famous cases of GLMs are: linear models, binomial and binary regression and Poisson regression. We’ll go through the GLM model specification and likelihood for all three. For linear models, we’ve developed them previously. The next two modules will be devoted to binomial and Poisson regression. We’ll only focus on the most popular and useful link functions.


## Linear models
* Linear models are the most useful applied statistical technique. However, they are not without their limitations.
  * Additive response models don't make much sense if the response is discrete, or stricly positive.
  * Additive error models often don't make sense, for example if the outcome has to be positive. 
  * Transformations are often hard to interpret. 
    * There's value in modeling the data on the scale that it was collected.
    * Particularly interpetable transformations, natural logarithms in specific,   aren't applicable for negative or zero values.



---
## Generalized linear models
* Introduced in a 1972 RSSB paper by Nelder and Wedderburn. 
* Involves three components
  * An *exponential family* model for the response.
  * A systematic component via a linear predictor.
  * A link function that connects the means of the response to the linear predictor.
  

---
## Example, linear models
* Assume that $Y_i \sim N(\mu_i, \sigma^2)$ (the Gaussian distribution is an exponential family distribution.)
* Define the linear predictor to be $\eta_i = \sum_{k=1}^p X_{ik} \beta_k$.
* The link function as $g$ so that $g(\mu) = \eta$.
  * For linear models $g(\mu) = \mu$ so that $\mu_i = \eta_i$
* This yields the same likelihood model as our additive error Gaussian linear model
$$
Y_i = \sum_{k=1}^p X_{ik} \beta_k + \epsilon_{i}
$$
where $\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)$

---
## Example, logistic regression
* Assume that $Y_i \sim Bernoulli(\mu_i)$ so that $E[Y_i] = \mu_i$ where $0\leq \mu_i \leq 1$.
* Linear predictor $\eta_i = \sum_{k=1}^p X_{ik} \beta_k$
* Link function 
$g(\mu) = \eta = \log\left( \frac{\mu}{1 - \mu}\right)$
$g$ is the (natural) log odds, referred to as the **logit**.
* Note then we can invert the logit function as
$$
\mu_i = \frac{\exp(\eta_i)}{1 + \exp(\eta_i)} ~~~\mbox{and}~~~
1 - \mu_i = \frac{1}{1 + \exp(\eta_i)}
$$
Thus the likelihood is
$$
\prod_{i=1}^n \mu_i^{y_i} (1 - \mu_i)^{1-y_i}
= \exp\left(\sum_{i=1}^n y_i \eta_i \right)
\prod_{i=1}^n (1 + \eta_i)^{-1}
$$

---
## Example, Poisson regression
* Assume that $Y_i \sim Poisson(\mu_i)$ so that $E[Y_i] = \mu_i$ where $0\leq \mu_i$
* Linear predictor $\eta_i = \sum_{k=1}^p X_{ik} \beta_k$
* Link function 
$g(\mu) = \eta = \log(\mu)$
* Recall that $e^x$ is the inverse of $\log(x)$ so that 
$$
\mu_i = e^{\eta_i}
$$
Thus, the likelihood is
$$
\prod_{i=1}^n (y_i !)^{-1} \mu_i^{y_i}e^{-\mu_i}
\propto \exp\left(\sum_{i=1}^n y_i \eta_i - \sum_{i=1}^n \mu_i\right)
$$

---
## Some things to note
* In each case, the only way in which the likelihood depends on the data is through 
$$
\sum_{i=1}^n y_i \eta_i =
\sum_{i=1}^n y_i\sum_{k=1}^p X_{ik} \beta_k = 
\sum_{k=1}^p \beta_k\sum_{i=1}^n X_{ik} y_i
$$
Thus if we don't need the full data, only $\sum_{i=1}^n X_{ik} y_i$. This simplification is a consequence of chosing so-called 'canonical' link functions.
* (This has to be derived). All models achieve their maximum at the root of the so called normal equations
$$
0=\sum_{i=1}^n \frac{(Y_i - \mu_i)}{Var(Y_i)}W_i
$$
where $W_i$ are the derivative of the inverse of the link function.

---
## About variances
$$
0=\sum_{i=1}^n \frac{(Y_i - \mu_i)}{Var(Y_i)}W_i
$$
* For the linear model $Var(Y_i) = \sigma^2$ is constant.
* For Bernoulli case $Var(Y_i) = \mu_i (1 - \mu_i)$
* For the Poisson case $Var(Y_i) = \mu_i$. 
* In the latter cases, it is often relevant to have a more flexible variance model, even if it doesn't correspond to an actual likelihood
$$
0=\sum_{i=1}^n \frac{(Y_i - \mu_i)}{\phi \mu_i (1 - \mu_i ) } W_i ~~~\mbox{and}~~~
0=\sum_{i=1}^n \frac{(Y_i - \mu_i)}{\phi \mu_i} W_i
$$
* These are called 'quasi-likelihood' normal equations 

---
## Odds and ends
* The normal equations have to be solved iteratively. Resulting in 
$\hat \beta_k$ and, if included, $\hat \phi$.
* Predicted linear predictor responses can be obtained as $\hat \eta = \sum_{k=1}^p X_k \hat \beta_k$
* Predicted mean responses as $\hat \mu = g^{-1}(\hat \eta)$
* Coefficients are interpretted as 
$$
g(E[Y | X_k = x_k + 1, X_{\sim k} = x_{\sim k}]) - g(E[Y | X_k = x_k, X_{\sim k}=x_{\sim k}]) = \beta_k
$$
or the change in the link function of the expected response per unit change in $X_k$ holding other regressors constant.
* Variations on Newon/Raphson's algorithm are used to do it.
* Asymptotics are used for inference usually. 
* Many of the ideas from linear models can be brought over to GLMs.





# Logistic Regression - for binary outcomes

Binary GLMs come from trying to model outcomes that can take only two values. Some examples include: survival or not at the end of a study, winning versus losing of a team and success versus failure of a treatment or product. Often these outcomes are called Bernoulli outcomes, from the Bernoulli distribution named after the famous probabilist and mathematician.

If we happen to have several exchangeable binary outcomes for the same level of covariate values, then that is binomial data and we can aggregate the 0’s and 1’s into the count of 1’s. As an example, imagine if we sprayed insect pests with 4 different pesticides and counted whether they died or not. Then for each spray, we could summarize the data with the count of dead and total number that were sprayed and treat the data as binomial rather than Bernoulli.

- alive/dead
- win/loss
- success/failure
- etc

called binary, bernoulli or 0/1 outcomes

Two cases:
binary and binomial regression. (binomial is done by same logistic regression in the special case where the covariate is just constant)



## Key ideas

* Frequently we care about outcomes that have two values
  * Alive/dead
  * Win/loss
  * Success/Failure
  * etc
* Called binary, Bernoulli or 0/1 outcomes 
* Collection of exchangeable binary outcomes for the same covariate data are called binomial outcomes.

---

## Example Baltimore Ravens win/loss
### Ravens Data

```{r loadRavens,cache=TRUE}
load("./data/ravensData.rda")
head(ravensData)
```

---

## Linear regression

$$ RW_i = b_0 + b_1 RS_i + e_i $$

$RW_i$ - 1 if a Ravens win, 0 if not

$RS_i$ - Number of points Ravens scored

$b_0$ - probability of a Ravens win if they score 0 points

$b_1$ - increase in probability of a Ravens win for each additional point

$e_i$ - residual variation due 

---

## Linear regression in R

this is of course not the logical thing to do.

```{r linearReg, cache=TRUE}
lmRavens <- lm(ravensData$ravenWinNum ~ ravensData$ravenScore)
summary(lmRavens)$coef
```


---

## Odds

__Binary Outcome 0/1__

$$RW_i$$  

__Probability (0,1)__

$$\rm{Pr}(RW_i | RS_i, b_0, b_1 )$$


__Odds $(0,\infty)$__
$$\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}$$ 

__Log odds $(-\infty,\infty)$__

$$\log\left(\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}\right)$$  


---

## Linear vs. logistic regression

__Linear__

$$ RW_i = b_0 + b_1 RS_i + e_i $$

or

$$ E[RW_i | RS_i, b_0, b_1] = b_0 + b_1 RS_i$$

__Logistic__

$$ \rm{Pr}(RW_i | RS_i, b_0, b_1) = \frac{\exp(b_0 + b_1 RS_i)}{1 + \exp(b_0 + b_1 RS_i)}$$

or

$$ \log\left(\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}\right) = b_0 + b_1 RS_i $$

---

## Interpreting Logistic Regression

$$ \log\left(\frac{\rm{Pr}(RW_i | RS_i, b_0, b_1 )}{1-\rm{Pr}(RW_i | RS_i, b_0, b_1)}\right) = b_0 + b_1 RS_i $$


$b_0$ - Log odds of a Ravens win if they score zero points

$b_1$ - Log odds ratio of win probability for each point scored (compared to zero points)

$\exp(b_1)$ - Odds ratio of win probability for each point scored (compared to zero points)

---
## Odds
- Imagine that you are playing a game where you flip a coin with success probability $p$.
- If it comes up heads, you win $X$. If it comes up tails, you lose $Y$.
- What should we set $X$ and $Y$ for the game to be fair?

    $$E[earnings]= X p - Y (1 - p) = 0$$
- Implies
    $$\frac{Y}{X} = \frac{p}{1 - p}$$    
- The odds can be said as "How much should you be willing to pay for a $p$ probability of winning a dollar?"
    - (If $p > 0.5$ you have to pay more if you lose than you get if you win.)
    - (If $p < 0.5$ you have to pay less if you lose than you get if you win.)

---
## Visualizing fitting logistic regression curves

```{r}
library(manipulate)
x <- seq(-10, 10, length = 1000)
manipulate(
    plot(x, exp(beta0 + beta1 * x) / (1 + exp(beta0 + beta1 * x)), 
         type = "l", lwd = 3, frame = FALSE),
    beta1 = slider(-2, 2, step = .1, initial = 2),
    beta0 = slider(-2, 2, step = .1, initial = 0)
    )
```

---

## Ravens logistic regression

```{r logReg, dependson = "loadRavens"}
logRegRavens <- glm(ravensData$ravenWinNum ~ ravensData$ravenScore,family="binomial")
summary(logRegRavens)
# if we had bounded count data, we would have to give it a sample size.
#by default it assumes the link function is the logit.

```
you want to look whether or not coefficient is close to 0 or not on the logit scale. on the exponentiated scale you want to check if it's close to 1.



---

## Ravens fitted values

```{r dependson = "logReg",fig.height=4,fig.width=4}
plot(ravensData$ravenScore,logRegRavens$fitted,pch=19,col="blue",xlab="Score",ylab="Prob Ravens Win")
#the fitted values are only showing a part of it. normally, the whole graph is same as the manipulate plot above.
```


---

## Odds ratios and confidence intervals

```{r dependson = "logReg",fig.height=4,fig.width=4}
exp(logRegRavens$coeff)
# 11% increase in the probability of winning whenever the ravens score.
exp(confint(logRegRavens))
```

our interval does contain 1. even though we know for sure that scoring points is what causes the Ravens to win the game, this coefficient turns out to not be significant.

---

## ANOVA for logistic regression

```{r dependson = "logReg",fig.height=4,fig.width=4}
anova(logRegRavens,test="Chisq")
```



---

## Interpreting Odds Ratios

* Not probabilities 
* Odds ratio of 1 = no difference in odds
* Log odds ratio of 0 = no difference in odds
* Odds ratio < 0.5 or > 2 commonly a "moderate effect"(or strong effect.)
* Relative risk $\frac{\rm{Pr}(RW_i | RS_i = 10)}{\rm{Pr}(RW_i | RS_i = 0)}$ often easier to interpret, harder to estimate
* For small probabilities RR $\approx$ OR but __they are not the same__!

[Wikipedia on Odds Ratio](http://en.wikipedia.org/wiki/Odds_ratio)


---

## Further resources

* [Wikipedia on Logistic Regression](http://en.wikipedia.org/wiki/Logistic_regression)
* [Logistic regression and glms in R](http://data.princeton.edu/R/glms.html)
* Brian Caffo's lecture notes on: [Simpson's paradox](http://ocw.jhsph.edu/courses/MethodsInBiostatisticsII/PDFs/lecture23.pdf), [Case-control studies](http://ocw.jhsph.edu/courses/MethodsInBiostatisticsII/PDFs/lecture24.pdf)
* [Open Intro Chapter on Logistic Regression](http://www.openintro.org/stat/down/oiStat2_08.pdf)







# count variables

Many data take the form of unbounded count data. For example, consider the number of calls to a call center or the number of flu cases in an area or the number of hits to a web site.

In some cases the counts are clearly bounded. However, modeling the counts as unbounded is often done when the upper limit is not known or very large relative to the number of events.

If the upper bound is known, the techniques we’re discussing can be used to model the proportion or rate. The starting point for most count analysis is the the Poisson distribution.

In the following lectures, we go over some of the basics of modeling count data.

## Key ideas

* Many data take the form of counts
  * Calls to a call center
  * Number of flu cases in an area
  * Number of cars that cross a bridge
* Data may also be in the form of rates
  * Percent of children passing a test
  * Percent of hits to a website from a country
* Linear regression with transformation is an option

---

## Poisson distribution
- The Poisson distribution is a useful model for counts and rates
- Here a rate is count per some monitoring time
- Some examples uses of the Poisson distribution
    - Modeling web traffic hits
    - Incidence rates
    - Approximating binomial probabilities with small $p$ and large $n$
    - Analyzing contigency table data

---
## The Poisson mass function
- $X \sim Poisson(t\lambda)$ if
$$
P(X = x) = \frac{(t\lambda)^x e^{-t\lambda}}{x!}
$$
For $x = 0, 1, \ldots$.
- The mean of the Poisson is $E[X] = t\lambda$, thus $E[X / t] = \lambda$
- The variance of the Poisson is $Var(X) = t\lambda$.
- The Poisson tends to a normal as $t\lambda$ gets large.

---

```{r simPois,fig.height=4,fig.width=8, cache=TRUE}
par(mfrow = c(1, 3))
plot(0 : 10, dpois(0 : 10, lambda = 2), type = "h", frame = FALSE)
plot(0 : 20, dpois(0 : 20, lambda = 10), type = "h", frame = FALSE)
plot(0 : 200, dpois(0 : 200, lambda = 100), type = "h", frame = FALSE) 
```

---

## Poisson distribution
### Sort of, showing that the mean and variance are equal
```{r}
x <- 0 : 10000; lambda = 3
mu <- sum(x * dpois(x, lambda = lambda))
sigmasq <- sum((x - mu)^2 * dpois(x, lambda = lambda))
c(mu, sigmasq)
```

---

## Example: Leek Group Website Traffic
* Consider the daily counts to Jeff Leek's web site

[http://biostat.jhsph.edu/~jleek/](http://biostat.jhsph.edu/~jleek/)

* Since the unit of time is always one day, set $t = 1$ and then
the Poisson mean is interpretted as web hits per day. (If we set $t = 24$, it would
be web hits per hour).

---

## Website data

```{r leekLoad,cache=TRUE}
#data is manually downloaded in the /data directory
load("./data/gaData.rda")
gaData$julian <- julian(gaData$date)
head(gaData)
#no data available
```

[http://skardhamar.github.com/rga/](http://skardhamar.github.com/rga/)


---

## Plot data

```{r, dependson="leekLoad",fig.height=4.5,fig.width=4.5}
plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")
```


---

## Linear regression

$$ NH_i = b_0 + b_1 JD_i + e_i $$

$NH_i$ - number of hits to the website

$JD_i$ - day of the year (Julian day)

$b_0$ - number of hits on Julian day 0 (1970-01-01)

$b_1$ - increase in number of hits per unit day

$e_i$ - variation due to everything we didn't measure


---

## Linear regression line

```{r linReg}
plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")
lm1 <- lm(gaData$visits ~ gaData$julian)
abline(lm1,col="red",lwd=3)
```
---

## Aside, taking the log of the outcome
- Taking the natural log of the outcome has a specific interpretation.
- Consider the model

$$ \log(NH_i) = b_0 + b_1 JD_i + e_i $$

$NH_i$ - number of hits to the website

$JD_i$ - day of the year (Julian day)

$b_0$ - log number of hits on Julian day 0 (1970-01-01)

$b_1$ - increase in log number of hits per unit day

$e_i$ - variation due to everything we didn't measure

---
## Exponentiating coefficients
- $e^{E[\log(Y)]}$ geometric mean of $Y$. 
    - With no covariates, this is estimated by $e^{\frac{1}{n}\sum_{i=1}^n \log(y_i)} = (\prod_{i=1}^n y_i)^{1/n}$
- When you take the natural log of outcomes and fit a regression model, your exponentiated coefficients
estimate things about geometric means.
- $e^{\beta_0}$ estimated geometric mean hits on day 0
- $e^{\beta_1}$ estimated relative increase or decrease in geometric mean hits per day
- There's a problem with logs with you have zero counts, adding a constant works


```{r}
round(exp(coef(lm(I(log(gaData$visits + 1)) ~ gaData$julian))), 5)
```



---

## Linear vs. Poisson regression

__Linear__

$$ NH_i = b_0 + b_1 JD_i + e_i $$

or

$$ E[NH_i | JD_i, b_0, b_1] = b_0 + b_1 JD_i$$

__Poisson/log-linear__

$$ \log\left(E[NH_i | JD_i, b_0, b_1]\right) = b_0 + b_1 JD_i $$

or

$$ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 + b_1 JD_i\right) $$


---

## Multiplicative differences

<br><br>
$$ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 + b_1 JD_i\right) $$

<br><br>

$$ E[NH_i | JD_i, b_0, b_1] = \exp\left(b_0 \right)\exp\left(b_1 JD_i\right) $$

<br><br>

If $JD_i$ is increased by one unit, $E[NH_i | JD_i, b_0, b_1]$ is multiplied by $\exp\left(b_1\right)$

---

## Poisson regression in R

```{r poisReg, dependson="linReg",fig.height=4.5,fig.width=4.5, cache=TRUE}
plot(gaData$julian,gaData$visits,pch=19,col="darkgrey",xlab="Julian",ylab="Visits")
glm1 <- glm(gaData$visits ~ gaData$julian,family="poisson")
abline(lm1,col="red",lwd=3); lines(gaData$julian,glm1$fitted,col="blue",lwd=3)
```


---

## Mean-variance relationship?

```{r, dependson="poisReg",fig.height=4.5,fig.width=4.5}
plot(glm1$fitted,glm1$residuals,pch=19,col="grey",ylab="Residuals",xlab="Fitted")
#the variance has to go up as the mean goes up. this is a problem.
```

---

## Model agnostic standard errors 

```{r agnostic}
#solution - quasi possion variables
#this model looks at variance as a constant multiple of the mean, rather than being equal to the mean
#but this is not the case here. it looks like we have this issue when there's larger variance for lower fitted values
library(sandwich) 
#sandwich variance estimator. invented in John's Hopkins
confint.agnostic <- function (object, parm, level = 0.95, ...)
{
    cf <- coef(object); pnames <- names(cf)
    if (missing(parm))
        parm <- pnames
    else if (is.numeric(parm))
        parm <- pnames[parm]
    a <- (1 - level)/2; a <- c(a, 1 - a)
    pct <- stats:::format.perc(a, 3)
    fac <- qnorm(a)
    ci <- array(NA, dim = c(length(parm), 2L), dimnames = list(parm,
                                                               pct))
    ses <- sqrt(diag(sandwich::vcovHC(object)))[parm]
    ci[] <- cf[parm] + ses %o% fac
    ci
}
```
[http://stackoverflow.com/questions/3817182/vcovhc-and-confidence-interval](http://stackoverflow.com/questions/3817182/vcovhc-and-confidence-interval)

very important applied topic. 
to understand whether your variance models hold.
---

## Estimating confidence intervals

```{r}
confint(glm1)
#if we don't do anything
confint.agnostic(glm1)
#here it doesn't make that big of a difference, but still the second one is the correct model here.
```


---

## Rates 


<br><br>


$$ E[NHSS_i | JD_i, b_0, b_1]/NH_i = \exp\left(b_0 + b_1 JD_i\right) $$
we want to actually interpret not the expected value of the outcome, but the expected value of the outcome divided by this relative term. in this case, the expected value of hits originating from the Simply Statistics page, divided by the total number of hits of JT Leek's general website.

<br><br>

$$ \log\left(E[NHSS_i | JD_i, b_0, b_1]\right) - \log(NH_i)  =  b_0 + b_1 JD_i $$

<br><br>

$$ \log\left(E[NHSS_i | JD_i, b_0, b_1]\right) = \log(NH_i) + b_0 + b_1 JD_i $$
it turns out this is all you have to do to add a regular proportion into a poission GLM. 

---

## Fitting rates in R 

```{r ratesFit,dependson="agnostic", cache=TRUE,fig.height=4,fig.width=4}
#adding the log(NHi) term to the model
glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
            family="poisson",data=gaData)
#visits + 1 because we can't take the log if 0.
#offset = log(visits + 1) this is how you add an offset term to the model. 
#alternatively, at the right side of the tilda you can use ~O(visits+1)
plot(julian(gaData$date),glm2$fitted,col="blue",pch=19,xlab="Date",ylab="Fitted Counts")
points(julian(gaData$date),glm1$fitted,col="red",pch=19)
```

blue points are adjusted for the red points.


---

## Fitting rates in R 

```{r,dependson="ratesFit",fig.height=4,fig.width=4}
glm2 <- glm(gaData$simplystats ~ julian(gaData$date),offset=log(visits+1),
            family="poisson",data=gaData)
plot(julian(gaData$date),gaData$simplystats/(gaData$visits+1),col="grey",xlab="Date",
     ylab="Fitted Rates",pch=19)
lines(julian(gaData$date),glm2$fitted/(gaData$visits+1),col="blue",lwd=3)
```
blue line is the fitted model. a lot of 0s early on, takes off after a while.
zero inflation is a serious problem here, also because it has a temporal component

---

## More information

* [Log-linear models and multiway tables](http://ww2.coastal.edu/kingw/statistics/R-tutorials/loglin.html)
* [Wikipedia on Poisson regression](http://en.wikipedia.org/wiki/Poisson_regression), [Wikipedia on overdispersion](http://en.wikipedia.org/wiki/Overdispersion)
* [Regression models for count data in R](http://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf)
* [pscl package](http://cran.r-project.org/web/packages/pscl/index.html) - the function _zeroinfl_ fits zero inflated models. 








Bonus
This lecture is a bit of an mishmash of interesting things that one can accomplish with linear models.


## How to fit functions using linear models
* Consider a model $Y_i = f(X_i) + \epsilon$. 
* How can we fit such a model using linear models (called scatterplot smoothing)
* Consider the model 

$$
  Y_i = \beta_0 + \beta_1 X_i + \sum_{k=1}^d (x_i - \xi_k)_+ \gamma_k + \epsilon_{i}
$$
  
where $(a)_+ = a$ if $a > 0$ and $0$ otherwise and $\xi_1 \leq ... \leq \xi_d$ are known knot points.
* Prove to yourelf that the mean function

$$
\beta_0 + \beta_1 X_i + \sum_{k=1}^d (x_i - \xi_k)_+ \gamma_k
$$

is continuous at the knot points.

---
## Simulated example
how to add knot terms
```{r, fig.height=4, fig.width=4}
n <- 500; x <- seq(0, 4 * pi, length = n); y <- sin(x) + rnorm(n, sd = .3)
knots <- seq(0, 8 * pi, length = 20); 
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot))
xMat <- cbind(1, x, splineTerms)
yhat <- predict(lm(y ~ xMat - 1))
plot(x, y, frame = FALSE, pch = 21, bg = "lightblue", cex = 2)
lines(x, yhat, col = "red", lwd = 2)
# sum(coef(mdl6)[2:3]) - iki knot point arasindaki slope coefficienti bulmak icin
# cikan coeffientlari topla
```
---
## Adding squared terms
* Adding squared terms makes it continuously differentiable at the knot points.
* Adding cubic terms makes it twice continuously differentiable at the knot points; etcetera.
$$
  Y_i = \beta_0 + \beta_1 X_i + \beta_2 X_i^2 + \sum_{k=1}^d (x_i - \xi_k)_+^2 \gamma_k + \epsilon_{i}
$$

---
```{r, fig.height=4, fig.width=4}  
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot)^2)
xMat <- cbind(1, x, x^2, splineTerms)
yhat <- predict(lm(y ~ xMat - 1))
plot(x, y, frame = FALSE, pch = 21, bg = "lightblue", cex = 2)
lines(x, yhat, col = "red", lwd = 2)
```

---
## Notes
* The collection of regressors is called a basis.
  * People have spent **a lot** of time thinking about bases for this kind of problem. So, consider this as just a teaser.
* Single knot point terms can fit hockey stick like processes.
* These bases can be used in GLMs as well.
* An issue with these approaches is the large number of parameters introduced. 
  * Requires some method of so called regularization.

---
## Harmonics using linear models
```{r}
##Chord finder, playing the white keys on a piano from octave c4 - c5
notes4 <- c(261.63, 293.66, 329.63, 349.23, 392.00, 440.00, 493.88, 523.25)
t <- seq(0, 2, by = .001); n <- length(t)
c4 <- sin(2 * pi * notes4[1] * t); e4 <- sin(2 * pi * notes4[3] * t); 
g4 <- sin(2 * pi * notes4[5] * t)
chord <- c4 + e4 + g4 + rnorm(n, 0, 0.3)
x <- sapply(notes4, function(freq) sin(2 * pi * freq * t))
fit <- lm(chord ~ x - 1)
```

---
```{r, fig.height=5,fig.width=5, echo=FALSE}
plot(c(0, 9), c(0, 1.5), xlab = "Note", ylab = "Coef^2", axes = FALSE, frame = TRUE, type = "n")
axis(2)
axis(1, at = 1 : 8, labels = c("c4", "d4", "e4", "f4", "g4", "a4", "b4", "c5"))
for (i in 1 : 8) abline(v = i, lwd = 3, col = grey(.8))
lines(c(0, 1 : 8, 9), c(0, coef(fit)^2, 0), type = "l", lwd = 3, col = "red")
```

---
```{r, fig.height=5, fig.wdith=5}
##(How you would really do it)
a <- fft(chord); plot(Re(a)^2, type = "l")
```

# Swirl

Variance Inflation Factors
Overfitting and Underfitting
Binary Outcomes
Count Outcomes

## Variance Inflation Factors

According to its documentation, the Swiss data set consists of a standardized
| fertility measure and socioeconomic indicators for each of 47 French-speaking
| provinces of Switzerland in about 1888 when Swiss fertility rates began to fall.
| Type head(swiss) or View(swiss) to examine the data.

> head(swiss)
             Fertility Agriculture Examination Education Catholic Infant.Mortality
Courtelary        80.2        17.0          15        12     9.96             22.2
Delemont          83.1        45.1           6         9    84.84             22.2
Franches-Mnt      92.5        39.7           5         5    93.40             20.2
Moutier           85.8        36.5          12         7    33.77             20.3
Neuveville        76.9        43.5          17        15     5.16             20.6
Porrentruy        76.1        35.3           9         7    90.57             26.6

| Nice work!

  |============================================                                |  58%
| Fertility was thought to depend on five socioeconomic factors: the percent of males
| working in Agriculture, the percent of draftees receiving the highest grade on the
| army's Examination, the percent of draftees with Education beyond primary school,
| the percent of the population which was Roman Catholic, and the rate of Infant
| Mortality in the province. Use linear regression to model Fertility in terms of
| these five regressors and an intercept. Store the model in a variable named mdl.

> mdl = lm(Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality, data = swiss)

| Perseverance, that's the answer.

  |================================================                            |  62%
| Calculate the VIF's for each of the regressors using vif(mdl).

> vif(mdl)
     Agriculture      Examination        Education         Catholic Infant.Mortality 
        2.284129         3.675420         2.774943         1.937160         1.107542 

| That's correct!

  |===================================================                         |  67%
| These VIF's show, for each regression coefficient, the variance inflation due to
| including all the others. For instance, the variance in the estimated coefficient
| of Education is 2.774943 times what it might have been if Education were not
| correlated with the other regressors. Since Education and score on an Examination
| are likely to be correlated, we might guess that most of the variance inflation for
| Education is due to including Examination.

# overfitting and underfitting

First, I would like to illustrate how omitting a correlated regressor can bias
| estimates of a coefficient. The relevant source code is in a file named fitting.R
| which I have copied into your working directory and tried to display in your source
| code editor. If I've failed to display it, you should open it manually.

...

  |========                                                                    |  11%
| Find the function simbias() at the top of fitting.R. Below the comment labeled
| Point A three regressors, x1, x2, and x3, are defined. Which of these two are
| correlated?

1: x1 and x2
2: x1 and x3
3: x2 and x3

Selection: 1

| Excellent work!

  |===========                                                                 |  15%
| Within simbias() another function, f(n), is defined. It forms a dependent variable,
| y, and at Point C returns the coefficient of x1 as estimated by two models, y ~ x1
| + x2, and y ~ x1 + x3. One regressor is missing in each model. In the expression
| for y (Point B,) what is the actual coefficient of x1?

1: 1
2: 1/sqrt(2)
3: 0.3

Selection: 1

| That's the answer I was looking for.

  |==============                                                              |  19%
| At Point D in simbias() the internal function, f(), is applied 150 times and the
| results returned as a 2x150 matrix. The first row of this matrix contains
| independent estimates of x1's coefficient in the case that x3, the regressor
| uncorrelated with x1, is omitted. The second row contains estimates of x1's
| coefficient when the correlated regressor, x2, is omitted. Use simbias(), accepting
| the default argument, to form these estimates and store the result in a variable
| called x1c. (The default argument just guarantees a nice histogram, in a figure to
| follow.)

> x1c <- simbias()

| You got it right!

  |=================                                                           |  22%
| The actual coefficient of x1 is 1. Having been warned that omitting a correlated
| regressor would bias estimates of x1's coefficient, we would expect the mean
| estimate of x1c's second row to be farther from 1 than the mean of x1c's first row.
| Using apply(x1c, 1, mean), find the means of each row.

> apply(x1c, 1 , mean)
      x1       x1 
1.034403 1.476944 

| You are quite good my friend!

  |====================                                                        |  26%
| Histograms of estimates from x1c's first row (blue) and second row (red) are shown.
| Estimates from the second row are clearly more than two standard deviations from
| the correct value of 1, and the bias due to omitting the correlated regressor is
| evident. (The code which produced this figure is incidental to the lesson, but is
| available as the function x1hist(), at the bottom of fitting.R.)

| Adding even irrelevant regressors can cause a model to tend toward a perfect fit.
| We illustrate this by adding random regressors to the swiss data and regressing on
| progressively more of them. As the number of regressors approaches the number of
| data points (47), the residual sum of squares, also known as the deviance,
| approaches 0. (The source code for this figure can be found as function bogus() in
| fitting.R.


| We'll now use anova to assess the significance of the two added regressors. The
| null hypothesis is that the added regressors are not significant. We'll explain in
| detail shortly, but right now just apply the significance test by entering
| anova(fit1, fit3).

### F statistic  - F test

> anova(fit1 , fit3)
Analysis of Variance Table

Model 1: Fertility ~ Agriculture
Model 2: Fertility ~ Agriculture + Examination + Education
  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    
1     45 6283.1                                  
2     43 3180.9  2    3102.2 20.968 4.407e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

| You are amazing!

  |=====================================                                       |  48%
| The three asterisks, ***, at the lower right of the printed table indicate that the
| null hypothesis is rejected at the 0.001 level, so at least one of the two
| additional regressors is significant. Rejection is based on a right-tailed F test,
| Pr(>F), applied to an F value. According to the table, what is that F value?


| An F statistic is a ratio of two sums of squares divided by their respective
| degrees of freedom. If the two scaled sums are independent and centrally
| chi-squared distributed with the same variance, the statistic will have an F
| distribution with parameters given by the two degrees of freedom. In our case, the
| two sums are residual sums of squares which, as we know, have mean zero hence are
| centrally chi-squared provided the residuals themselves are normally distributed.
| The two relevant sums are given in the RSS (Residual Sum of Squares) column of the
| table. What are they?

1: 45 and 43
2: 2 and 3102.2
3: 6283.1 and 3180.9

Selection: 3

| Keep up the great work!

  |==========================================                                  |  56%
| R's function, deviance(model), calculates the residual sum of squares, also known
| as the deviance, of the linear model given as its argument. Using deviance(fit3),
| verify that 3180.9 is fit3's residual sum of squares. (Of course, fit3 is called
| Model 2 in the table.)


# calculating the F value

In the next several steps, we will show how to calculate the F value, 20.968, which
| appears in the table printed by anova(). We'll begin with the denominator, which is
| fit3's residual sum of squares divided by its degrees of freedom. Fit3 has 43
| residual degrees of freedom. This figure is obtained by subtracting 4, the the
| number of fit3's predictors (the 3 named and the intercept,) from 47, the number of
| samples in swiss. Store the value of deviance(fit3)/43 in a variable named d.


 d <- deviance(fit3)/43

| That's correct!

  |================================================                            |  63%
| The numerator is the difference, deviance(fit1)-deviance(fit3), divided by the
| difference in the residual degrees of freedom of fit1 and fit3, namely 2. This
| calculation requires some theoretical justification which we omit, but the
| essential idea is that fit3 has 2 predictors in addition to those of fit1.
| Calculate the numerator and store it in a variable named n.

n <- (deviance(fit1)- deviance(fit3))/2

| Keep up the great work!

  |===================================================                         |  67%
| Calculate the ratio, n/d, to show it is essentially equal to the F value, 20.968,
| given by anova().

20.96783

| You are quite good my friend!

  |=====================================================                       |  70%
| We'll now calculate the p-value, which is the probability that a value of n/d or
| larger would be drawn from an F distribution which has parameters 2 and 43. This
| value was given as 4.407e-07 in the column labeled Pr(>F) in the table printed by
| anova(), a very unlikely value if the null hypothesis were true. Calculate this
| p-value using pf(n/d, 2, 43, lower.tail=FALSE).

pf(n/d, 2, 43, lower.tail  = FALSE)
[1] 4.406913e-07

| Excellent job!

  |========================================================                    |  74%
| Based on the calculated p-value, a false rejection of the null hypothesis is
| extremely unlikely. We are confident that fit3 is significantly better than fit1,
| with one caveat: analysis of variance is sensitive to its assumption that model
| residuals are approximately normal. If they are not, we could get a small p-value
| for that reason. It is thus worth testing residuals for normality. The Shapiro-Wilk
| test is quick and easy in R. Normality is its null hypothesis. Use
| shapiro.test(fit3$residuals) to test the residual of fit3.

shapiro.test(fit3$residuals)

	Shapiro-Wilk normality test

data:  fit3$residuals
W = 0.97276, p-value = 0.336


| That's correct!

  |===========================================================                 |  78%
| The Shapiro-Wilk p-value of 0.336 fails to reject normality, supporting confidence
| in our analysis of variance. In order to illustrate the use of anova() with more
| than two models, I have constructed fit5 and fit6 using the first 5 and all 6
| regressors (including the intercept) respectively. Thus fit1, fit3, fit5, and fit6
| form a nested sequence of models; the regressors of one are included in those of
| the next. Enter anova(fit1, fit3, fit5, fit6) at the R prompt now to get the
| flavor.





# binary outcomes

 Of course, we would expect a real curve to be smoother. We would not, for instance,
| expect the Ravens to win half the games in which they scored zero points, nor to
| win all the games in which they scored more than 28. A generalized linear model
| which has these properties supposes that the log odds of a win depend linearly on
| the score. That is, log(p/(1-p)) = b0 + b1*score. The link function, log(p/(1-p)),
| is called the logit, and the process of finding the best b0 and b1, is called
| logistic regression.

...

  |===================================                                         |  46%
| The "best" b0 and b1 are those which maximize the likelihood of the actual win/loss
| record. Based on the score of a game, b0 and b1 give us a log odds, which we can
| convert to a probability, p, of a win. We would like p to be high for the scores of
| winning games, and low for the scores of losses.

...

  |======================================                                      |  50%
| We can use R's glm() function to find the b0 and b1 which maximize the likelihood
| of our observations. Referring back to the data frame, we want to predict the
| binary outcomes, ravenWinNum, from the points scored, ravenScore. This corresponds
| to the formula, ravenWinNum ~ ravenScore, which is the first argument to glm. The
| second argument, family, describes the outcomes, which in our case are binomial.
| The third argument is the data, ravenData. Call glm with these parameters and store
| the result in a variable named mdl.


The probabilities estimated by logistic regression using glm() are represented by
| the black curve. It is more reasonable than our crude estimate in several respects:
| It increases smoothly with score, it estimates that 15 points give the Ravens a 50%
| chance of winning, that 28 points give them an 80% chance, and that 55 points make
| a win very likely (98%) but not absolutely certain.

...

  |============================================                                |  58%
| The model is less credible at scores lower than 9. Of course, there is no data in
| that region; the Ravens scored at least 9 points in every game. The model gives
| them a 33% chance of winning if they score 9 points, which may be reasonable, but
| it also gives them a 16% chance of winning even if they score no points! We can use
| R's predict() function to see the model's estimates for lower scores. The function
| will take mdl and a data frame of scores as arguments and will return log odds for
| the give scores. Call predict(mdl, data.frame(ravenScore=c(0, 3, 6))) and store the
| result in a variable called lodds.


| Since predict() gives us log odds, we will have to convert to probabilities. To
| convert log odds to probabilities use exp(lodds)/(1+exp(lodds)). Don't bother to
| store the result in a variable. We won't need it.


Linear regression minimizes the squared difference between predicted and actual
| observations, i.e., minimizes the variance of the residual. If an additional
| predictor significantly reduces the residual's variance, the predictor is deemed
| important. Deviance extends this idea to generalized linear regression, using
| (negative) log likelihoods in place of variance. (For a detailed explanation, see
| the slides and lectures.) To see the analysis of deviance for our model, type
| anova(mdl).

> anova(mdl)
Analysis of Deviance Table

Model: binomial, link: logit

Response: ravenWinNum

Terms added sequentially (first to last)


           Df Deviance Resid. Df Resid. Dev
NULL                          19     24.435
ravenScore  1   3.5398        18     20.895

| That's the answer I was looking for.

  |======================================================================      |  92%
| The value, 3.5398, labeled as the deviance of ravenScore, is actually the
| difference between the deviance of our model, which includes a slope, and that of a
| model which includes only an intercept, b0. This value is centrally chi-square
| distributed (for large samples) with 1 degree of freedom (2 parameters minus 1
| parameter, or equivalently 19-18.) The null hypothesis is that the coefficient of
| ravenScore is zero. To confidently reject this hypothesis, we would want 3.5398 to
| be larger than the 95th percentile of chi-square distribution with one degree of
| freedom. Use qchisq(0.95, 1) to compute the threshold of this percentile.


# count outcomes

 The counts generated by a Poisson process are, strictly speaking, slightly
| different than the normalized sums of the Central Limit Theorem. However, the
| counts in a given period of time will represent sums of larger numbers of terms as
| lambda increases. In fact, it can be formally shown that for large lambda a Poisson
| distribution is well approximated by a normal. The figure illustrates this effect.
| It shows progression from a sparse, asymetric, Poisson probability mass function on
| the left, to a dense, bell-shaped curve on the right as lambda varies from 2 to
| 100.


quickly checking the class of a column

| Our dates are represented in terms of R's class, Date. Verify this by typing
| class(hits[,'date']), or something equivalent.

> class(hits[, 'date'])


 The figure suggests that our Poisson regression fits the data very well. The black
| line is the estimated lambda, or mean number of visits per day. We see that mean
| visits per day increased from around 5 in early 2011 to around 10 by 2012, and to
| around 20 by late 2013. It is approximately doubling every year.

...

  |=================================                                           |  44%
| Type summary(mdl) to examine the estimated coefficients and their significance.

> summary(mdl)

Call:
glm(formula = visits ~ date, family = poisson, data = hits)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-5.0466  -1.5908  -0.3198   0.9128  10.6545  

Coefficients:
              Estimate Std. Error z value Pr(>|z|)    
(Intercept) -3.275e+01  8.130e-01  -40.28   <2e-16 ***
date         2.293e-03  5.266e-05   43.55   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 5150.0  on 730  degrees of freedom
Residual deviance: 3121.6  on 729  degrees of freedom
AIC: 6069.6

Number of Fisher Scoring iterations: 5


| You got it right!

  |====================================                                        |  47%
| Both coefficients are significant, being far more than two standard errors from
| zero. The Residual deviance is also very significantly less than the Null,
| indicating a strong effect. (Recall that the difference between Null and Residual
| deviance is approximately chi-square with 1 degree of freedom.) The Intercept
| coefficient, b0, just represents log average hits on R's Date 0, namely January 1,
| 1970. We will ignore it and focus on the coefficient of date, b1, since exp(b1)
| will estimate the percentage at which average visits increase per day of the site's
| life.

Get the 95% confidence interval for exp(b1) by exponentiating confint(mdl, 'date')

> exp(confint(mdl, 'date'))
Waiting for profiling to be done...
   2.5 %   97.5 % 
1.002192 1.002399 

| Keep up the great work!

  |========================================                                    |  53%
| Visits are estimated to increase by a factor of between 1.002192 and 1.002399 per
| day. That is, between 0.2192% and 0.2399% per day. This actually represents more
| than a doubling every year.


Our model looks like a pretty good description of the data, but no model is perfect
| and we can often learn about a data generation process by looking for a model's
| shortcomings. As shown in the figure, one thing about our model is 'zero inflation'
| in the first two weeks of January 2011, before the site had any visits. The model
| systematically overestimates the number of visits during this time. A less obvious
| thing is that the standard deviation of the data may be increasing with lambda
| faster than a Poisson model allows. This possibility can be seen in the rightmost
| plot by visually comparing the spread of green dots with the standard deviation
| predicted by the model (black dashes.) Also, there are four or five bursts of
| popularity during which the number of visits far exceeds two standard deviations
| over average. Perhaps these are due to mentions on another site.


 In the figure, the maximum number of visits occurred in late 2012. Visits from the
| Simply Statistics blog were also at their maximum that day. To find the exact date
| we can use which.max(hits[,'visits']). Do this now.

> which.max(hits[, 'visits'])
[1] 704

| You are really on a roll!

  |==================================================                          |  66%
| The maximum number of visits is recorded in row 704 of our data frame. Print that
| row by typing hits[704,].


The maximum number of visits, 94, occurred on December 4, 2012, of which 64 came
| from the Simply Statistics blog. We might consider the 64 visits to be a special
| event, over and above normal. Can the difference, 94-64=30 visits, be attributed to
| normal traffic as estimated by our model? To check, we will need the value of
| lambda on December 4, 2012. This will be entry 704 of the fitted.values element of
| our model. Extract mdl$fitted.values[704] and store it in a variable named lambda.

> lambda <- mdl$fitted.values[704]

| All that hard work is paying off!

  |=======================================================                     |  72%
| The number of visits explained by our model on December 4, 2012 are those of a
| Poisson random variable with mean lambda. We can find the 95th percentile of this
| distribution using qpois(.95, lambda). Try this now.

> qois(.95, lambda)
Error in qois(0.95, lambda) : could not find function "qois"
> qpois(.95, lambda)
[1] 33

| You got it!

  |=========================================================                   |  75%
| So, 95% of the time we would see 33 or fewer visits, hence 30 visits would not be
| rare according to our model. It would seem that on December 4, 2012, the very high
| number of visits was due to references from Simply Statistics. To gauge the
| importance of references from Simply Statistics we may wish to model the proportion
| of traffic such references represent. Doing so will also illustrate the use of
| glm's parameter, offset, to model frequencies and proportions.

...

  |===========================================================                 |  78%
| A Poisson process generates counts, and counts are whole numbers, 0, 1, 2, 3, etc.
| A proportion is a fraction. So how can a Poisson process model a proportion? The
| trick is to include the denominator of the fraction, or more precisely its log, as
| an offset. Recall that in our data set, 'simplystats' is the visits from Simply
| Statistics, and 'visits' is the total number of visits. We would like to model the
| fraction simplystats/visits, but to avoid division by zero we'll actually use
| simplystats/(visits+1). A Poisson model assumes that log(lambda) is a linear
| combination of predictors. Suppose we assume that log(lambda) = log(visits+1) + b0
| + b1*date. In other words, if we insist that the coefficient of log(visits+1) be
| equal to 1, we are predicting the log of mean visits from Simply Statistics as a
| proportion of total visits: log(lambda/(visits+1)) = b0 + b1*date.

glm's parameter, offset, has precisely this effect. It fixes the coefficient of the
| offset to 1. To create a model for the proportion of visits from Simply Statistics,
| we let offset=log(visits+1). Create such a Poisson model now and store it as a
| variable called mdl2.

mdl2 <- glm(simplystats ~ date , family = poisson, offset = log(visits+1), data = hits)

| Excellent job!

  |================================================================            |  84%
| Although summary(mdl2) will show that the estimated coefficients are significantly
| different than zero, the model is actually not impressive. We can illustrate why by
| looking at December 4, 2012, once again. On that day there were 64 actual visits
| from Simply Statistics. However, according to mdl2, 64 visits would be extremely
| unlikely. You can verify this weakness in the model by finding mdl2's 95th
| percentile for that day. Recalling that December 4, 2012 was sample 704, find
| qpois(.95, mdl2$fitted.values[704]).




------------------------------------------
# Chapter 8 exercises from caffo book

```{r}
library(datasets)
data(Seatbelts)
seatbelts <- as.data.frame(Seatbelts)
#because in the original datasets no. of people killed for 0 kms driven is not very meaningful(because the range of kms variables is very big, and no. of people killed for 0 pounds of petrol price is not very meaningful because petrol price is created with a retail index, we change and scale them a little to make sense)
#also, kms is >0. centering it allows a better understanding
library(dplyr)
seatbelts = mutate(seatbelts,
                   #standardized Petrol Price
                   pp = (PetrolPrice - mean(PetrolPrice))/sd(PetrolPrice),
                   #megameters
                   mm = kms/1000,
                   mmc = mm - mean(mm))
head(seatbelts)
```

```{r}
fit <- lm(DriversKilled ~ pp + mmc, seatbelts)
summary(fit)
```
we're expecting 7 fewer deaths in 1 std. deviation change in petrol price, holding the kms constant.

we're expecting around 2 fewer deaths for every additional 1000 kms drivven, holding the pp constant.

question2
```{r}
#we just want to predict
fit2 <- lm(DriversKilled ~ kms + PetrolPrice, data = seatbelts)
summary(fit2)$coef
predict(fit2, newdata = data.frame(kms = mean(seatbelts$kms), PetrolPrice = mean(seatbelts$PetrolPrice)), interval = "prediction")
```


question3

```{r}
ey <- resid(lm(DriversKilled ~ kms, data = seatbelts))
ex <- resid(lm(PetrolPrice ~ kms, data = seatbelts))
summary(lm(ey ~ ex - 1))$coef
```

```{r}
summary(fit2)$coef
```

identical coefficients of petrol price variable

question4

```{r}
edk <- resid(lm(DriversKilled ~ PetrolPrice, data = seatbelts))
ekm <- resid(lm(kms ~ PetrolPrice, data = seatbelts))
summary(lm(edk ~ ekm - 1))$coef
summary(fit2)$coef
```

Binary Problems

```{r}
library(datasets)
library(dplyr)
data(Seatbelts)
seatbelts = as.data.frame(Seatbelts)
seatbelts = mutate(seatbelts,
                   dkb = 1 * (DriversKilled >119),
                   pp = (PetrolPrice - mean(PetrolPrice))/sd(PetrolPrice),
                   mm = kms/1000,
                   mmc = mm- mean(mm))
table(seatbelts$dkb)

```


```{r}
fit = glm(dkb ~ pp + mmc + law, family = "binomial", data = seatbelts)
summary(fit)$coef
round(summary(fit)$coef, 3)
```

logit scale odds of having > 119 drivers killed that month is 0.003 lower for every 1000 kms driven.

```{r}
#law coefficient
exp(-0.615522450)
```

There was a 54% more odds of deaths before the law was enacted.

conversely
```{r}
1 - exp(-0.615522450)
```

There was a 45% decrease in odds of deaths after the law was enacted. Holding the other variables constant.

```{r}
exp(-0.002938343)
```

.3% decrease in the odds of greater than 119 drivers killed that month for every additional 1000 kms driven that month

```{r}
fit2 <- glm(cbind(DriversKilled, drivers - DriversKilled) ~ pp + mmc + law, family = binomial, data = seatbelts)
summary(fit2)
```

anova for model comparison

```{r}
fit_1 = glm(dkb ~ law, family = "binomial", data = seatbelts)
fit_2 = glm(dkb ~ law + pp, family = binomial, data = seatbelts)
fit_3 = glm(dkb ~ law + pp + mmc, family = binomial, data = seatbelts)
anova(fit_1, fit_2, fit_3)
```



Examples and tricks section 

question 1

```{r}
library(datasets)
data(Seatbelts)
seatbelts <- as.data.frame(Seatbelts)
#because in the original datasets no. of people killed for 0 kms driven is not very meaningful(because the range of kms variables is very big, and no. of people killed for 0 pounds of petrol price is not very meaningful because petrol price is created with a retail index, we change and scale them a little to make sense)
#also, kms is >0. centering it allows a better understanding
library(dplyr)
seatbelts = mutate(seatbelts,
                   #standardized Petrol Price
                   pp = (PetrolPrice - mean(PetrolPrice))/sd(PetrolPrice),
                   #megameters
                   mm = kms/1000,
                   mmc = mm - mean(mm))
head(seatbelts)
```

```{r}
fit = lm(DriversKilled ~ mmc + pp, data = seatbelts)
summary(fit)
```

```{r}
fit2 = lm(I(log(DriversKilled)) ~ mmc + pp, data = seatbelts)
summary(fit2)
```

question 3

```{r}
seatbelts$lawFactor <- NULL
for(i in 1:nrow(seatbelts)) {
  if(seatbelts$law[i] == 0){
    seatbelts$lawFactor[i] <- "No" 
  } else {
    seatbelts$lawFactor[i] <- "Yes"
  }
}

seatbelts$lawFactor <- relevel(as.factor(seatbelts$lawFactor), "Yes")

fit3 <- lm(DriversKilled ~ mmc + pp + lawFactor, data = seatbelts)
summary(fit3)
```


question 4

```{r}
seatbelts = mutate(seatbelts,
                   #standardized Petrol Price
                   pp = (PetrolPrice - mean(PetrolPrice))/sd(PetrolPrice),
                   ppf = as.factor((pp <= -1.5) + (pp <= 0) + (pp <= 1.5) + (pp < Inf)),
                   #megameters
                   mm = kms/1000,
                   mmc = mm - mean(mm))
head(seatbelts)
summary(lm(DriversKilled ~ ppf + mmc+ law, data = seatbelts))
```

Adjustment Exercises

1)

```{r}
library(datasets)
data(Seatbelts)
head(Seatbelts)
seatbelts = as.data.frame(Seatbelts)
head(seatbelts)
library(dplyr)
seatbelts <- mutate(seatbelts,
                    pp = (PetrolPrice - mean(PetrolPrice))/sd(PetrolPrice),
                    mm = kms / 1000,
                    mmc = mm - mean(mm))
head(seatbelts)
```

```{r}
fit1 = lm(DriversKilled ~ mmc, data = seatbelts)
fit2 = lm(DriversKilled ~ mmc + pp, data = seatbelts)
sumCoef1 <- summary(fit1)
sumCoef2 <- summary(fit2)
sumCoef1$coefficients
sumCoef2$coefficients
```

```{r}
library(ggplot2)
qplot(x= seatbelts$mmc, y = sumCoef1$residuals) + geom_hline(yintercept = 0, col = "red") 
```

```{r}
qplot(x= seatbelts$mmc, y = sumCoef2$residuals) + geom_hline(yintercept = 0, col = "red") 
```

```{r}
anova(fit1, fit2)
```

```{r}
fit3 = lm(DriversKilled ~ pp, data = seatbelts)
fit4 = lm(DriversKilled ~ pp + mmc, data = seatbelts)
sumCoef3 = summary(fit3)
sumCoef4 = summary(fit4)
sumCoef3$coefficients
sumCoef4$coefficients
```

```{r}
qplot(x= seatbelts$mmc, y = sumCoef3$residuals) + geom_hline(yintercept = 0, col = "red") 
```

```{r}
qplot(x= seatbelts$mmc, y = sumCoef4$residuals) + geom_hline(yintercept = 0, col = "red") 
```

```{r}
cor(seatbelts$pp, seatbelts$mmc)
```

```{r}
par(mfrow = c(2,2 ))
plot(fit2)
```

```{r}
library(datasets)
data(Seatbelts)
seatbelts = as.data.frame(Seatbelts)
library(dplyr)
seatbelts <- mutate(seatbelts,
                    pp = (PetrolPrice - mean(PetrolPrice))/sd(PetrolPrice),
                    mm = kms / 1000,
                    mmc = mm - mean(mm))
```

```{r}
fit = lm(DriversKilled ~ mmc + pp + law, data = seatbelts)
summary(fit)
```

```{r}

par(mfrow = c(2, 2))
plot(fit)
```

```{r}
myfits = dffits(fit)
mybetas = dfbetas(fit)
summary(myfits)
summary(mybetas)
```


Multiple Variables and model selection question 2
```{r}
fit2 = lm(DriversKilled ~ mmc + pp, data = seatbelts)
fit3 = lm(DriversKilled ~ mmc, data = seatbelts)
anova(fit3, fit2, fit)

```
RSS gives you the Residual sum of Squares. so the sum of sq column gives you the difference compared to the line above.

DF the degrees of freedom of the test. if there are 2 additional variables in the second lm, then it's 2.