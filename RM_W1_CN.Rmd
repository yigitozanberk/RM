---
title: "Regression Models Week 1"
author: "Yigit Ozan Berk"
date: "7/4/2019"
output: html_document
---

# Week 1

Syllabus

Module 1, least squares and linear regression

01_01 Introduction
01_02 Notation
01_03 Ordinary least squares
01_04 Regression to the mean
01_05 Linear regression
01_06 Residuals
01_07 Regression inference
Module 2, Multivariable regression

02_01 Multivariate regression
02_02 Multivariate examples
02_03 Adjustment
02_04 Residual variation and diagnostics
02_05 Multiple variables
Module 3, Generalized linear models

03_01 GLMs
03_02 Binary outcomes
03_03 Count outcomes
03_04 Olio
Module 4, Logistic Regression and Poisson Regression

Logistic Regression
Poisson Regression
Hodgepodge

```{r}
library(swirl)
install_from_swirl("Regression Models")
```

- *simply statistics* blog. check it out.[https://simplystatistics.org]


typical regression questions:

- to use the parents' heights to predict childrens' heights.
- to try to find a parsimonious, easily described mean relationship between parent and children's heights.
- to investigate the variation in childrens' heights that appears unrelated to parents' heights (residual variation).
- to quantify what impact genotype information has beyond parental height in explaining child height.
- to figure out how/whether and what assumptions are needed to generalize findings beyond the data in question.
- why do children of very tall parents tend to be tall, but a little shorter than their parents and why children of very short parents tend to be short, but a little taller than their parents? (this is a famous question called 'Regression to the Mean')

## basic least squares

```{r}
library(UsingR)
```

data used by francis galton in 1885. ahead of his time. invented regression and correlation.

- parent distribution is all heterosexual couples
- correction for gender via multiplying female heights by 1.08
- overplotting is an issue from discretization

```{r}
data(galton)
library(reshape)
long <- melt(galton)
g <- ggplot(long, aes(x = value, fill = variable))
g <- g + geom_histogram(colour = "black", binwidth = 1)
g <- g + facet_grid(. ~ variable)
g

```


```{r}
library(manipulate)
myHist <- function(mu) {
        mse <- mean((galton$child - mu)^2)
        #mean squared error
        g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "salmon", colour = "black", binwidth = 1)
        g <- g + geom_vline(xintercept = mu, size = 3)
        g <- g + ggtitle(paste("mu = ", mu, ", MSE = ", round(mse, 2), sep = ""))
        g
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
```


index.rmd for r markdowns of every lecture in github repositories for all lectures for all slides


```{r}
ggplot(galton, aes(x = parent,
                   y = child)) + geom_point()
```

not very informative

```{r}
library(dplyr)
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")                    
g
```

kucuk belli olmuyor buyuk ebatli bak plot'a

size and colour of the point represents number of parent child combination at that point.

only the units are missing in this plot.


The regression line generation

```{r}
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
myPlot <- function(beta){
    g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
    g <- g  + scale_size(range = c(2, 20), guide = "none" )
    g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
    g <- g + geom_point(aes(colour=freq, size = freq))
    g <- g + scale_colour_gradient(low = "lightblue", high="white")                     
    g <- g + geom_abline(intercept = 0, slope = beta, size = 3)
    mse <- mean( (y - beta * x) ^2 )
    g <- g + ggtitle(paste("beta = ", beta, "mse = ", round(mse, 3)))
    g
}
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
```

as the beta coefficient gets closer to 0.64 the mse gets smaller.

the solution:

```{r}
lm(I(child - mean(child)) ~ I(parent - mean(parent)) - 1, data = galton)

# -1 says get rid of the intercept because we're talking about regression through the origin.
```

next several lectures are about this solution.

# Linear least squares

Ordinary least squares (OLS) is the workhorse of statistics. It gives a way of taking complicated outcomes and explaining behavior (such as trends) using linearity. The simplest application of OLS is fitting a line through some data. In the next few lectures, we cover the basics of linear least squares.


Coding example

```{r}
library(UsingR)
data(galton)
library(dplyr); library(ggplot2)
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g + scale_size(range = c(2, 20), guide = "none")
# guide none ne demek
g <- g + geom_point(colour = "grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour = freq, size  = freq))
g <- g + scale_colour_gradient(low = "lightblue", high = "white")
g
```


```{r}
y <- galton$child
x <- galton$parent
beta1 <- cor(y,x) * sd(y)/sd(x)
beta0 <- mean(y) - beta1*mean(x)
rbind(c(beta0, beta1), coef(lm(y~x)))

```

```{r}
lm(y ~ x)
```


```{r}
#reversing the outcome/predictor relationship
beta1 <- cor(y,x) * sd(x)/sd(y)
beta0 <- mean(x) - beta1*mean(y)
rbind(c(beta0, beta1), coef(lm(x ~ y)))
```

Centered line
```{r}
yc <- y - mean(y)
xc <- x - mean(x)
beta1 <- sum(yc * xc) / sum(xc ^ 2)
c(beta1, coef(lm(y ~ x))[2])
#2nd coefficient is the slope


```

```{r}
lm(yc ~ xc - 1)
# you put -1 to get rid of the intercept
```

Normalizing variables
```{r}
yn <- (y - mean(y))/sd(y)
xn <- (x - mean(x))/sd(x)
c(cor(y,x), cor(yn, xn), coef(lm(yn ~ xn))[2])
```

```{r}
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g + scale_size(range = c(2, 20), guide = "none")
# guide none ne demek
g <- g + geom_point(colour = "grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour = freq, size  = freq))
g <- g + scale_colour_gradient(low = "lightblue", high = "white")
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g
```


# Regression to the Mean

Here is a fundamental question. Why is it that the children of tall parents tend to be tall, but not as tall as their parents? Why do children of short parents tend to be short, but not as short as their parents? Conversely, why do parents of very short children, tend to be short, but not a short as their child? And the same with parents of very tall children?

We can try this with anything that is measured with error. Why do the best performing athletes this year tend to do a little worse the following? Why do the best performers on hard exams always do a little worse on the next hard exam?

These phenomena are all examples of so-called regression to the mean. Regression to the mean, was invented by Francis Galton in the paper “Regression towards mediocrity in hereditary stature” The Journal of the Anthropological Institute of Great Britain and Ireland , Vol. 15, (1886). The idea served as a foundation for the discovery of linear regression.



```{r}
set.seed(1)
x <- rnorm(100)
y <- rnorm(100)
odr<- order(x)
x[odr[100]] # maximum X


```

```{r}
y[odr[100]] # the y paired with x
```

 
 
```{r}
library(UsingR)
library(ggplot2)
data(father.son)
y <- (father.son$sheight - mean(father.son$sheight))/sd(father.son$sheight)
x <- (father.son$fheight - mean(father.son$fheight))/ sd(father.son$fheight)
rho <- cor(x,y)
g <- ggplot(data.frame(x = x, y = y), aes(x = x, y = y))
g <- g + geom_point(size = 6, colour = "black", alpha = 0.2)
g <- g + geom_point(size = 4, colour = "salmon", alpha = 0.2)
g <- g + xlim(-4, 4) + ylim(-4 , 4)
#should be all encompassing
g <- g + geom_abline(intercept = 0, slope = 1)
g <- g + geom_vline(xintercept = 0)
g <- g + geom_hline(yintercept = 0)
g <- g + geom_abline(intercept = 0, slope = rho, size = 2)
g <- g + geom_abline(intercept = 0, slope = 1/rho, size = 2)
g

```
 
 father's height X
 son's height Y
 
 if there is no noise, we would predict the son's height as the same.
 
 lot's of noise in the data. so the prediction is now not the same, but on the prediction line(cor(x,y))
 
 that is the regression to the mean.
 
 how shrunken this correlation is towards horizontal line, gives you the extent of the regression to the mean.
 
 if the son's height is the predictor, and the father's height is the outcome, the slope is flipped.
 
 
 # Swirl
 

```{r}
library(swirl)
install_from_swirl("Regression Models")
```
 
 
 For the first part of this course you should complete the following lessons:

- Introduction
- Residuals
- Least Squares Estimation


 
