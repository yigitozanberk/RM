---
title: "Regression Models Week 1"
author: "Yigit Ozan Berk"
date: "7/4/2019"
output: html_document
---

# Week 1

Syllabus

Module 1, least squares and linear regression

01_01 Introduction
01_02 Notation
01_03 Ordinary least squares
01_04 Regression to the mean
01_05 Linear regression
01_06 Residuals
01_07 Regression inference
Module 2, Multivariable regression

02_01 Multivariate regression
02_02 Multivariate examples
02_03 Adjustment
02_04 Residual variation and diagnostics
02_05 Multiple variables
Module 3, Generalized linear models

03_01 GLMs
03_02 Binary outcomes
03_03 Count outcomes
03_04 Olio
Module 4, Logistic Regression and Poisson Regression

Logistic Regression
Poisson Regression
Hodgepodge

```{r}
library(swirl)
install_from_swirl("Regression Models")
```

- *simply statistics* blog. check it out.[https://simplystatistics.org]


typical regression questions:

- to use the parents' heights to predict childrens' heights.
- to try to find a parsimonious, easily described mean relationship between parent and children's heights.
- to investigate the variation in childrens' heights that appears unrelated to parents' heights (residual variation).
- to quantify what impact genotype information has beyond parental height in explaining child height.
- to figure out how/whether and what assumptions are needed to generalize findings beyond the data in question.
- why do children of very tall parents tend to be tall, but a little shorter than their parents and why children of very short parents tend to be short, but a little taller than their parents? (this is a famous question called 'Regression to the Mean')

## basic least squares

```{r}
library(UsingR)
```

data used by francis galton in 1885. ahead of his time. invented regression and correlation.

- parent distribution is all heterosexual couples
- correction for gender via multiplying female heights by 1.08
- overplotting is an issue from discretization

```{r}
data(galton)
library(reshape)
long <- melt(galton)
g <- ggplot(long, aes(x = value, fill = variable))
g <- g + geom_histogram(colour = "black", binwidth = 1)
g <- g + facet_grid(. ~ variable)
g

```


```{r}
library(manipulate)
myHist <- function(mu) {
        mse <- mean((galton$child - mu)^2)
        #mean squared error
        g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "salmon", colour = "black", binwidth = 1)
        g <- g + geom_vline(xintercept = mu, size = 3)
        g <- g + ggtitle(paste("mu = ", mu, ", MSE = ", round(mse, 2), sep = ""))
        g
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
```


index.rmd for r markdowns of every lecture in github repositories for all lectures for all slides


```{r}
ggplot(galton, aes(x = parent,
                   y = child)) + geom_point()
```

not very informative

```{r}
library(dplyr)
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")                    
g
```

kucuk belli olmuyor buyuk ebatli bak plot'a

size and colour of the point represents number of parent child combination at that point.

only the units are missing in this plot.


The regression line generation

```{r}
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
myPlot <- function(beta){
    g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
    g <- g  + scale_size(range = c(2, 20), guide = "none" )
    g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
    g <- g + geom_point(aes(colour=freq, size = freq))
    g <- g + scale_colour_gradient(low = "lightblue", high="white")                     
    g <- g + geom_abline(intercept = 0, slope = beta, size = 3)
    mse <- mean( (y - beta * x) ^2 )
    g <- g + ggtitle(paste("beta = ", beta, "mse = ", round(mse, 3)))
    g
}
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
```

as the beta coefficient gets closer to 0.64 the mse gets smaller.

the solution:

```{r}
lm(I(child - mean(child)) ~ I(parent - mean(parent)) - 1, data = galton)

# -1 says get rid of the intercept because we're talking about regression through the origin.
```

next several lectures are about this solution.

# Linear least squares

Ordinary least squares (OLS) is the workhorse of statistics. It gives a way of taking complicated outcomes and explaining behavior (such as trends) using linearity. The simplest application of OLS is fitting a line through some data. In the next few lectures, we cover the basics of linear least squares.


