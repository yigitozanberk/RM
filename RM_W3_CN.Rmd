---
title: "Regression Modeling Week 3"
author: "Yigit Ozan Berk"
date: "7/17/2019"
output: html_document
---

- multivariable regression
- multivariable regression tips and tricks
- adjustment
- residuals again
- model selection
- swirl excercises :
multivar examples 2
multivar examples 3
residual diagnostics and variation
- practice exercise in regression modeling

# Multivariable regression

if you have lots and lots and lots of variables, you may find a "related" regressor by chance. That's the problem of multiplicity. Keep that in mind. 

multivariable regression tries to look at the relationship of a predictor and a response, while having, at some level, accounted for other variables.

smoker - mint usage - lung function example. you have to keep the smoker status constant to see if mint usage *really* has an effect on lung function


CAGO competition

multivariable regression is also a great predictor.

simple linear regression is not equipped to handle more than one predictor.

model selection strategies, avoiding overfitting

multivariable prediction is a pretty good starting point.

- what are the consequences of adding lots of regressors? overfitting. zero residuals.

```{r}
set.seed(100)
n = 100; x = rnorm(n); x2 = rnorm(n) ; x3 = rnorm(n)
y = 1 + x + x2 + x3 + rnorm(n, sd = .1)
ey = resid(lm(y ~ x2 + x3))
ex = resid(lm(x ~ x2 + x3))
#remember lm by default takes an intercept.
sum(ey * ex) / sum(ex ^ 2)
coef(lm(ey ~ ex - 1))
coef(lm(y ~ x + x2 + x3))
```

x term is exactly the same with regression through origin estimate with the residuals


- linears models are the single most important applied statistical and machine learning technique, by far.

some amazing things that you can accomplish with linear models:

- decompose a signal into its harmonics
- flexibly fit complicated functions
- fit factor variables as predictors
- uncover complex multivariate relationships with the response
- build accurate prediction models.

# Multivariable regression tips and tricks

Swiss data set

```{r}
require(datasets)
data(swiss)
?swiss
```

what explains fertility in this province?

```{r}
install.packages("GGally")
require(GGally)
#add on tools for ggplot
require(ggplot2)
my_fn <- function(data, mapping, method="loess", ...){
      p <- ggplot(data = data, mapping = mapping) + 
      geom_point() + 
      geom_smooth(method=method, ...)
      p
    }

# Default loess curve    
g = ggpairs(swiss[1:4], lower = list(continuous = wrap(my_fn, method="loess")))
g
```


(Check the GGally package)

```{r}
summary(lm(Fertility ~ ., data = swiss))$coefficients
#period means all the variables. (linearly)
#all variables are in terms of percentages
```

interpretation:

(for Agriculture)
estimate: we expect a 0.17 decrease (because it's negative) in standardized fertility for every 1% increase in percentage of males involved in agriculture in holding the remaining variables constant.

std. error: the statistical variability of the coefficient.

testing H_0 : beta_agri = 0 versus H_a : beta_agri != 0
take the estimate: -0.1721140 
subtract off the hypothesized value : -0.1721149 - 0
divide by the std. error of the estimate : (-0.1721149 - 0)/0.07030392
this is the t statistic!

we could calculate the probability of getting a t statistic as extreme as that.

```{r}
sumCoef <- summary(lm(Fertility ~ ., data = swiss))$coefficients
pt(sumCoef[2,3], 41)
#degrees of freedom is n - numberOfCoefficients = 41 (5 variable + 1 intercept)
```

```{r}
#double the t value for two sided result
pt(sumCoef[2,3], 41) * 2
```

```{r}
#this is the same as the 4th column!
sumCoef[2,4]
```



Let's go through some other models, and see how the process of model selection changes our estimates.

```{r}
summary(lm(Fertility ~ Agriculture, data = swiss))$coefficients
```

```{r}
summary(lm(Fertility ~ Agriculture, data = swiss))
```

The coefficient changed signes.

This is the impact of Simpson's *Perceived* Paradox.

Regression is a dynamic process, you have to think about which variables to include.

If there hasn't been a randomization process done to protect you from confounding: 
You have to go through a scientific dynamic process of putting confounders in and out and thinking about what they are doing to your effective interest in order to evaluate it.

```{r}
n = 100; x2 = 1:n; x1 = .01 * x2 + runif(n, -.1, .1); y = -x1 + x2 + rnorm(n, sd = .01)
# x1 grows linearly with x2, also has some random noise
# y is negatively associated with x1. positively associated with x2. and has random noise
summary(lm(y ~ x1))$coef
summary(lm(y ~ x1 + x2))$coef
```

it's sort of picking up the residual effect of x2 in x1 in the first model. if we fit the correct model, we get the correct coefficients.

what is regression doing(in the second model)? it's taking x1, and removing the linear effect of x2. then looking at the  correlation.


```{r}
dat = data.frame(y = y, x1 = x1, x2 = x2, ey = resid(lm(y ~ x2)), ex1 = resid(lm(x1 ~ x2)))
g = ggplot(dat, aes(y = y, x= x1, colour = x2))
g = g + geom_point(colour = "grey50", size = 5) + geom_smooth(method = lm, se = FALSE, colour = "black") + geom_point(size = 4)
g
```

there is clearly positive relationship at first look. but you can also see that x2 is also positively related with y. so there is confounding. that's what's happening here.

let's see what happens if we plot the residuals

```{r}
g2 = ggplot(dat, aes(y = ey, x = ex1, colour= x2))
g2 = g2 + geom_point(colour = "grey50", size = 5) + geom_smooth(method = lm, se = FALSE, colour = "black") + geom_point(size = 4)
g2
```

you can see that for the residual y and residual x1, there's a clear negative relationship. and the slope is roughly -1.

x2 variable is clearly not related to residual x1 variable.

the linear model is removing the x2 from both x1 and y. and that's why you get this correct relationship here.

This doesn't mean throwing every variable into your model is correct. there's consequences to throwing in extra variables. unnecessary variables.


### back to the swiss dataset

- the sign(of Agriculture) reverses itself with the inclusion of Examination and Education
- the percent of males in the province working in agriculture is negatively related to educational attainment (correlation of -0.6395) and Education and Examination (correlation of 0.6984) are obviously measuring similar things.

- is the positive marginal an artifact for not having accounted for these other variables? (Education does have a stronger effect on fertility by the way)

- at the minimum, anyone claiming that provinces that are more agricultural have higher fertility rates would immediately be open to criticism.


what happens if you include a *completely unnecessary* variable to the model?
(not a random noise)

```{r}
z <- swiss$Agriculture + swiss$Education
lm(Fertility ~ . + z, data = swiss)
```

if you see an NA in R after you fit a linear model, most likely reason is because it is either numerically or exactly a linear combination of the other variables.


# Multivariable regression examples part 2

```{r}
require(datasets); data("InsectSprays");require(stats)
library(ggplot2)
g = ggplot(data = InsectSprays, aes(y = count, x = spray, fill = spray))
g = g + geom_violin(colour = "black", size = 2)
g = g + xlab("Type of spray") + ylab("Insect count")
g
```

```{r}
head(InsectSprays)
```


```{r}
sumCoef <- summary(lm(count ~ spray , data = InsectSprays))
sumCoef
```

Spray A is missing. so everything is in comparison to spray A.

the intercept is the mean for spray A.

Spray C estimate is the average difference of means of spray A and spray C. (14.5000 -12.4167)

If we want to compare spray B and spray C; 0.8333 - 12.4167. That wouldn't give us standard erros immediately, but we would have estimates. !!!!!

## How to pick the reference level

```{r}
sumCoef2 = summary(lm(count ~
                        I(1 * (spray == "B")) + I(1 * (spray == "C")) + 
                            I( 1 * (spray == "D")) + I(1 * (spray == "E")) +
                                 I( 1 * (spray == "F")), 
                                    data = InsectSprays))
sumCoef2$coef
# identical results
```

```{r}
#taking spray B as reference level.
sumCoef3 = summary(lm(count ~
                        I(1 * (spray == "A")) + I(1 * (spray == "C")) + 
                            I( 1 * (spray == "D")) + I(1 * (spray == "E")) +
                                 I( 1 * (spray == "F")), 
                                    data = InsectSprays))
sumCoef3$coef
```

now we also have standard errors.

## what if we include all 6?

```{r}
lm(count ~
                        I(1 * (spray == "B")) + I(1 * (spray == "C")) + 
                            I( 1 * (spray == "D")) + I(1 * (spray == "E")) +
                                 I( 1 * (spray == "F")) + I(1 * (spray == "A")), 
                                    data = InsectSprays)

```

it gives NA for spray A, the 6th regression parameter. because it's redundant. 
we have 6 means for 6 parameters plus the intercept. now I have 6 means to fit seven parameters(including intercept), it can't do that so it drops one.

What if I want the coefficients not to be compared to one another? the coefficients to be the mean for each group?

## omitting the intercept.

```{r}
library(dplyr)
summary(lm( count ~ spray - 1, data = InsectSprays))$coef
```
these are tesing wether spray A has killed any insects.(H_0 : beta_A == 0) 
the previous models were testing whether sprayB insect kill was different from sprayA insect kill, or whether sprayC insect kill was different from sprayA insect kill.

```{r}
summarise(group_by(InsectSprays, spray), mn = mean(count))
```

now all groups are presented with their means. because the regression function now has 6 parameters and 6 means to work with.

if you include the intercept to the function, the intercept becomes spray A, and the other coefficients are referenced to spray A - with standard errors.

How you insert the factor variables to lm() is very important for interpretation.

## reordering the levels easily = relevel()
```{r}
spray2 <- relevel(InsectSprays$spray, "C")
summary(lm(count ~ spray2, data = InsectSprays))$coef
```

this is so much faster than typing in I(1 * (spray == "C")) over and over again!


### Summary

if we treat spray as a factor, R includes an intercept and omits the alphabetically first level of the factor.

- all t-tests are for comparisons of sprays versus spray A
- empirical mean for A isthe intercept
- other group means are the itc plus their coefficient.

if we omit an intercept, then it includes terms for all levels of factors
- group mean are the coefficients
- tests are tests of whether the groups are different than zero


## other thoughts on this data

- counts are bounded from below by 0, violates the assumption of normal data.
it would be more natural to assume the data was poisson or at least over dispersed poisson. 

the variance is clearly not constant, as our regression models would assume. so our estimates are correct but our inferences are definitely not.

for more advanced topics you need more classes than this specialization tree (like for heteroscedasticity)

perhaps taking logs of the counts would help. 

-there are 0 counts present, so maybe log(Count + 1)

we'll cover Poisson GLMs for fitting count data later on.



