---
title: "Regression Modeling Week 3"
author: "Yigit Ozan Berk"
date: "7/17/2019"
output: html_document
---

- multivariable regression
- multivariable regression tips and tricks
- adjustment
- residuals again
- model selection
- swirl excercises :
multivar examples 2
multivar examples 3
residual diagnostics and variation
- practice exercise in regression modeling

# Multivariable regression

if you have lots and lots and lots of variables, you may find a "related" regressor by chance. That's the problem of multiplicity. Keep that in mind. 

multivariable regression tries to look at the relationship of a predictor and a response, while having, at some level, accounted for other variables.

smoker - mint usage - lung function example. you have to keep the smoker status constant to see if mint usage *really* has an effect on lung function


CAGO competition

multivariable regression is also a great predictor.

simple linear regression is not equipped to handle more than one predictor.

model selection strategies, avoiding overfitting

multivariable prediction is a pretty good starting point.

- what are the consequences of adding lots of regressors? overfitting. zero residuals.

```{r}
set.seed(100)
n = 100; x = rnorm(n); x2 = rnorm(n) ; x3 = rnorm(n)
y = 1 + x + x2 + x3 + rnorm(n, sd = .1)
ey = resid(lm(y ~ x2 + x3))
ex = resid(lm(x ~ x2 + x3))
#remember lm by default takes an intercept.
sum(ey * ex) / sum(ex ^ 2)
coef(lm(ey ~ ex - 1))
coef(lm(y ~ x + x2 + x3))
```

x term is exactly the same with regression through origin estimate with the residuals


- linears models are the single most important applied statistical and machine learning technique, by far.

some amazing things that you can accomplish with linear models:

- decompose a signal into its harmonics
- flexibly fit complicated functions
- fit factor variables as predictors
- uncover complex multivariate relationships with the response
- build accurate prediction models.

# Multivariable regression tips and tricks

Swiss data set

```{r}
require(datasets)
data(swiss)
?swiss
```

what explains fertility in this province?

```{r}
install.packages("GGally")
require(GGally)
#add on tools for ggplot
require(ggplot2)
my_fn <- function(data, mapping, method="loess", ...){
      p <- ggplot(data = data, mapping = mapping) + 
      geom_point() + 
      geom_smooth(method=method, ...)
      p
    }

# Default loess curve    
g = ggpairs(swiss[1:4], lower = list(continuous = wrap(my_fn, method="loess")))
g
```


(Check the GGally package)

```{r}
summary(lm(Fertility ~ ., data = swiss))$coefficients
#period means all the variables. (linearly)
#all variables are in terms of percentages
```

interpretation:

(for Agriculture)
estimate: we expect a 0.17 decrease (because it's negative) in standardized fertility for every 1% increase in percentage of males involved in agriculture in holding the remaining variables constant.

std. error: the statistical variability of the coefficient.

testing H_0 : beta_agri = 0 versus H_a : beta_agri != 0
take the estimate: -0.1721140 
subtract off the hypothesized value : -0.1721149 - 0
divide by the std. error of the estimate : (-0.1721149 - 0)/0.07030392
this is the t statistic!

we could calculate the probability of getting a t statistic as extreme as that.

```{r}
sumCoef <- summary(lm(Fertility ~ ., data = swiss))$coefficients
pt(sumCoef[2,3], 41)
#degrees of freedom is n - numberOfCoefficients = 41 (5 variable + 1 intercept)
```

```{r}
#double the t value for two sided result
pt(sumCoef[2,3], 41) * 2
```

```{r}
#this is the same as the 4th column!
sumCoef[2,4]
```



Let's go through some other models, and see how the process of model selection changes our estimates.

```{r}
summary(lm(Fertility ~ Agriculture, data = swiss))$coefficients
```

```{r}
summary(lm(Fertility ~ Agriculture, data = swiss))
```

The coefficient changed signes.

This is the impact of Simpson's *Perceived* Paradox.

Regression is a dynamic process, you have to think about which variables to include.

If there hasn't been a randomization process done to protect you from confounding: 
You have to go through a scientific dynamic process of putting confounders in and out and thinking about what they are doing to your effective interest in order to evaluate it.

```{r}
n = 100; x2 = 1:n; x1 = .01 * x2 + runif(n, -.1, .1); y = -x1 + x2 + rnorm(n, sd = .01)
# x1 grows linearly with x2, also has some random noise
# y is negatively associated with x1. positively associated with x2. and has random noise
summary(lm(y ~ x1))$coef
summary(lm(y ~ x1 + x2))$coef
```

it's sort of picking up the residual effect of x2 in x1 in the first model. if we fit the correct model, we get the correct coefficients.

what is regression doing(in the second model)? it's taking x1, and removing the linear effect of x2. then looking at the  correlation.


```{r}
dat = data.frame(y = y, x1 = x1, x2 = x2, ey = resid(lm(y ~ x2)), ex1 = resid(lm(x1 ~ x2)))
g = ggplot(dat, aes(y = y, x= x1, colour = x2))
g = g + geom_point(colour = "grey50", size = 5) + geom_smooth(method = lm, se = FALSE, colour = "black") + geom_point(size = 4)
g
```

there is clearly positive relationship at first look. but you can also see that x2 is also positively related with y. so there is confounding. that's what's happening here.

let's see what happens if we plot the residuals

```{r}
g2 = ggplot(dat, aes(y = ey, x = ex1, colour= x2))
g2 = g2 + geom_point(colour = "grey50", size = 5) + geom_smooth(method = lm, se = FALSE, colour = "black") + geom_point(size = 4)
g2
```

you can see that for the residual y and residual x1, there's a clear negative relationship. and the slope is roughly -1.

x2 variable is clearly not related to residual x1 variable.

the linear model is removing the x2 from both x1 and y. and that's why you get this correct relationship here.

This doesn't mean throwing every variable into your model is correct. there's consequences to throwing in extra variables. unnecessary variables.


### back to the swiss dataset

- the sign(of Agriculture) reverses itself with the inclusion of Examination and Education
- the percent of males in the province working in agriculture is negatively related to educational attainment (correlation of -0.6395) and Education and Examination (correlation of 0.6984) are obviously measuring similar things.

- is the positive marginal an artifact for not having accounted for these other variables? (Education does have a stronger effect on fertility by the way)

- at the minimum, anyone claiming that provinces that are more agricultural have higher fertility rates would immediately be open to criticism.


what happens if you include a *completely unnecessary* variable to the model?
(not a random noise)

```{r}
z <- swiss$Agriculture + swiss$Education
lm(Fertility ~ . + z, data = swiss)
```

if you see an NA in R after you fit a linear model, most likely reason is because it is either numerically or exactly a linear combination of the other variables.


# Multivariable regression examples part 2

```{r}
require(datasets); data("InsectSprays");require(stats)
library(ggplot2)
g = ggplot(data = InsectSprays, aes(y = count, x = spray, fill = spray))
g = g + geom_violin(colour = "black", size = 2)
g = g + xlab("Type of spray") + ylab("Insect count")
g
```

```{r}
head(InsectSprays)
```


```{r}
sumCoef <- summary(lm(count ~ spray , data = InsectSprays))
sumCoef
```

Spray A is missing. so everything is in comparison to spray A.

the intercept is the mean for spray A.

Spray C estimate is the average difference of means of spray A and spray C. (14.5000 -12.4167)

If we want to compare spray B and spray C; 0.8333 - 12.4167. That wouldn't give us standard erros immediately, but we would have estimates. !!!!!

## How to pick the reference level

```{r}
sumCoef2 = summary(lm(count ~
                        I(1 * (spray == "B")) + I(1 * (spray == "C")) + 
                            I( 1 * (spray == "D")) + I(1 * (spray == "E")) +
                                 I( 1 * (spray == "F")), 
                                    data = InsectSprays))
sumCoef2$coef
# identical results
```

```{r}
#taking spray B as reference level.
sumCoef3 = summary(lm(count ~
                        I(1 * (spray == "A")) + I(1 * (spray == "C")) + 
                            I( 1 * (spray == "D")) + I(1 * (spray == "E")) +
                                 I( 1 * (spray == "F")), 
                                    data = InsectSprays))
sumCoef3$coef
```

now we also have standard errors.

## what if we include all 6?

```{r}
lm(count ~
                        I(1 * (spray == "B")) + I(1 * (spray == "C")) + 
                            I( 1 * (spray == "D")) + I(1 * (spray == "E")) +
                                 I( 1 * (spray == "F")) + I(1 * (spray == "A")), 
                                    data = InsectSprays)

```

it gives NA for spray A, the 6th regression parameter. because it's redundant. 
we have 6 means for 6 parameters plus the intercept. now I have 6 means to fit seven parameters(including intercept), it can't do that so it drops one.

What if I want the coefficients not to be compared to one another? the coefficients to be the mean for each group?

## omitting the intercept.

```{r}
library(dplyr)
summary(lm( count ~ spray - 1, data = InsectSprays))$coef
```
these are tesing wether spray A has killed any insects.(H_0 : beta_A == 0) 
the previous models were testing whether sprayB insect kill was different from sprayA insect kill, or whether sprayC insect kill was different from sprayA insect kill.

```{r}
summarise(group_by(InsectSprays, spray), mn = mean(count))
```

now all groups are presented with their means. because the regression function now has 6 parameters and 6 means to work with.

if you include the intercept to the function, the intercept becomes spray A, and the other coefficients are referenced to spray A - with standard errors.

How you insert the factor variables to lm() is very important for interpretation.

## reordering the levels easily = relevel()
```{r}
spray2 <- relevel(InsectSprays$spray, "C")
summary(lm(count ~ spray2, data = InsectSprays))$coef
```

this is so much faster than typing in I(1 * (spray == "C")) over and over again!


### Summary

if we treat spray as a factor, R includes an intercept and omits the alphabetically first level of the factor.

- all t-tests are for comparisons of sprays versus spray A
- empirical mean for A isthe intercept
- other group means are the itc plus their coefficient.

if we omit an intercept, then it includes terms for all levels of factors
- group mean are the coefficients
- tests are tests of whether the groups are different than zero


## other thoughts on this data

- counts are bounded from below by 0, violates the assumption of normal data.
it would be more natural to assume the data was poisson or at least over dispersed poisson. 

the variance is clearly not constant, as our regression models would assume. so our estimates are correct but our inferences are definitely not.

for more advanced topics you need more classes than this specialization tree (like for heteroscedasticity)

perhaps taking logs of the counts would help. 

-there are 0 counts present, so maybe log(Count + 1)

we'll cover Poisson GLMs for fitting count data later on.

ANCOVA example

```{r}
library(dplyr)
library(datasets) ; data(swiss)
head(swiss)
```

### dealing with bimodal variables

```{r}
hist(swiss$Catholic)
```


majority catholic or majority protestant. very bimodal

```{r}
swiss = mutate(swiss, CatholicBin = 1 * (Catholic > 50))

#here we are making the province fully catholic if the majority is catholic, or visa versa. and this truly makes the variable with two levels.(factor variable)
head(swiss)
```

```{r}
# ders notlari: RM W3 23/7/19
library(ggplot2)
g = ggplot(swiss, aes(x = Agriculture, y = Fertility, colour = factor(CatholicBin)))
g = g + geom_point(size = 6, colour = "black") + geom_point(size = 4)
g  = g + xlab("% in Agriculture") + ylab("Fertility")
g
```

there are some potential issues here because of the outliers. it will be dealt with in a future class. these are ignored for the time being.

there will be two separate lines. for catholics and protestants.

Model:

Y = Fertility
X_1 = Agriculture
X_2 = { 1, for over 50% Catholic
        0, otherwise}
        
```{r}
#first model
fit = lm(Fertility ~ Agriculture, data = swiss)
g1 = g
g1 = g1 + geom_abline(intercept = coef(fit)[1], slope = coef(fit)[2], size = 2)
g1
# the first model doesn't include religion at all
```

```{r}
summary(fit)$coef
```

```{r}
#second model
fit2 = lm(Fertility ~ Agriculture + factor(CatholicBin), data = swiss)
summary(fit2)$coef
#because the value of CatholicBin is either 0 or 1, you don't have to call factor(CatholicBin), but it's appropriate for context. if there is even one more different value, R is going to treat it as a continuous regressor, and it will distrupt the regression.
```
change in the intercept by CatholicBin variable is estimated as 7.88 with std. error of 3.7483 within the 95 confidence interval.

```{r}
g2 = g
g2 = g2 + geom_abline(intercept = coef(fit2)[1], slope = coef(fit2)[2], size = 2)
g2 = g2 + geom_abline(intercept = coef(fit2)[1] + coef(fit2)[3], slope = coef(fit2)[2], size = 2)
g2

```

```{r}
#third model
fit3 = lm(Fertility ~ Agriculture * factor(CatholicBin), data = swiss)
summary(fit3)$coef
#what does the asteriks do?
# this is our third model. agriculture slope is the mostly protestant provinces. the agriculture:factor(cath..) slope is the mostly catholic provinces.
# when you add the asteriks, it automatically fits the interaction plus all the main effects. (intercept, agriculture, factor(CatholicBin), agriculture:factor(catholicBin))

```
```{r}
g3 = g
g3 = g3 + geom_abline(intercept = coef(fit3)[1], slope = coef(fit3)[2], size = 2)
g3 = g3 + geom_abline(intercept = coef(fit3)[1] + coef(fit3)[3], 
                      slope = coef(fit3)[2] + coef(fit3)[4],
                      size = 2)
g3
```

for the blue dots, it's not clear what the low left corner blue dots mean, but it's for the following lectures.

# Adjustment

Adjustment, is the idea of putting regressors into a linear model to investigate the role of a third variable on the relationship between another two. Since it is often the case that a third variable can distort, or confound if you will, the relationship between two others.

As an example, consider looking at lung cancer rates and breath mint usage. For the sake of completeness, imagine if you were looking at forced expiratory volume (a measure of lung function) and breath mint usage. If you found a statistically significant regression relationship, it wouldn’t be wise to rush off to the newspapers with the headline “Breath mint usage causes shortness of breath!”, for a variety of reasons. First off, even if the association is sound, you don’t know that it’s causal. But, more importantly in this case, the likely culprit is smoking habits. Smoking rates are likely related to both breath mint usage rates and lung function. How would you defend your finding against the accusation that it’s just variability in smoking habits?

If your finding held up among non-smokers and smokers analyzed separately, then you might have something. In other words, people wouldn’t even begin to believe this finding unless it held up while holding smoking status constant. That is the idea of adding a regression variable into a model as adjustment. The coefficient of interest is interpreted as the effect of the predictor on the response, holding the adjustment variable constant.

In this lecture, we’ll use simulation to investigate how adding a regressor into a model addresses the idea of adjustment.



```{r}
n <- 100; t <- rep(c(0, 1), c(n/2, n/2)); x <- c(runif(n/2), runif(n/2));
beta0 <- 0; beta1 <- 2; tau <- 1; sigma <- .2
y <- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma)
plot(x, y, type = "n", frame = FALSE)
abline(lm(y ~ x), lwd = 2)
abline(h = mean(y[1 : (n/2)]), lwd = 3)
abline(h = mean(y[(n/2 + 1) : n]), lwd = 3)
fit <- lm(y ~ x + t)
abline(coef(fit)[1], coef(fit)[2], lwd = 3)
abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)
points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = "black", bg = "lightblue", cex = 2)
points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = "black", bg = "salmon", cex = 2)
```




### Simulation 1

{r, fig.height=5, fig.width=5, echo = FALSE, results='hide'}

```{r}
n <- 100; t <- rep(c(0, 1), c(n/2, n/2)); x <- c(runif(n/2), runif(n/2));
beta0 <- 0; beta1 <- 2; tau <- 1; sigma <- .2
y <- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma)
plot(x, y, type = "n", frame = FALSE)
abline(lm(y ~ x), lwd = 2)
abline(h = mean(y[1 : (n/2)]), lwd = 3)
abline(h = mean(y[(n/2 + 1) : n]), lwd = 3)
fit <- lm(y ~ x + t)
abline(coef(fit)[1], coef(fit)[2], lwd = 3)
abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)
points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = "black", bg = "lightblue", cex = 2)
points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = "black", bg = "salmon", cex = 2)
```

horizontal lines show the marginal effect of group status, disregarding x (red and blue groups) upper line would be the mean for red group, lower line would be the mean for blue group

but there's a pretty clear linear relationship between the outcome and the regressor.

Y = beta_0 + beta_1 * T + beta_2 * X + e



### Some things to note in this simulation
* The X variable is unrelated to group status
* The X variable is related to Y, but the intercept depends
  on group status.
* The group variable is related to Y.
  * The relationship between group status and Y is constant depending on X.
  * The relationship between group and Y disregarding X is about the same as holding X constant


### Simulation 2

```{r}
n <- 100; t <- rep(c(0, 1), c(n/2, n/2)); x <- c(runif(n/2), 1.5 + runif(n/2));
beta0 <- 0; beta1 <- 2; tau <- 0; sigma <- .2
y <- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma)
plot(x, y, type = "n", frame = FALSE)
abline(lm(y ~ x), lwd = 2)
abline(h = mean(y[1 : (n/2)]), lwd = 3)
abline(h = mean(y[(n/2 + 1) : n]), lwd = 3)
fit <- lm(y ~ x + t)
abline(coef(fit)[1], coef(fit)[2], lwd = 3)
abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)
points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = "black", bg = "lightblue", cex = 2)
points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = "black", bg = "salmon", cex = 2)
```
massive treatment effect, but nothing if you count X. ("propensity score")



### Some things to note in this simulation
* The X variable is highly related to group status
* The X variable is related to Y, the intercept
  doesn't depend on the group variable.
  * The X variable remains related to Y holding group status constant
* The group variable is marginally related to Y disregarding X.
* The model would estimate no adjusted effect due to group.
  * There isn't any data to inform the relationship between
    group and Y.
  * This conclusion is entirely based on the model.
  
this is what makes observational data analysis very hard compared to randomized trials.


### Simulation 3

```{r}
n <- 100; t <- rep(c(0, 1), c(n/2, n/2)); x <- c(runif(n/2), .9 + runif(n/2));
beta0 <- 0; beta1 <- 2; tau <- -1; sigma <- .2
y <- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma)
plot(x, y, type = "n", frame = FALSE)
abline(lm(y ~ x), lwd = 2)
abline(h = mean(y[1 : (n/2)]), lwd = 3)
abline(h = mean(y[(n/2 + 1) : n]), lwd = 3)
fit <- lm(y ~ x + t)
abline(coef(fit)[1], coef(fit)[2], lwd = 3)
abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)
points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = "black", bg = "lightblue", cex = 2)
points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = "black", bg = "salmon", cex = 2)
```
there is some overlap, direct evidence. 

but, it also is a hard case, because if you look, the marginal means are far, but more importantly mean_red > mean_blue. but when we fit our model, we see adjusted blue mean is higher than red = mean_blue > mean_red.

This is called the simpson's paradox.


### Some things to note in this simulation
* Marginal association has red group higher than blue.
* Adjusted relationship has blue group higher than red.
* Group status related to X.
* There is some direct evidence for comparing red and blue
holding X fixed.


### Simulation 4
```{r}
n <- 100; t <- rep(c(0, 1), c(n/2, n/2)); x <- c(.5 + runif(n/2), runif(n/2));
beta0 <- 0; beta1 <- 2; tau <- 1; sigma <- .2
y <- beta0 + x * beta1 + t * tau + rnorm(n, sd = sigma)
plot(x, y, type = "n", frame = FALSE)
abline(lm(y ~ x), lwd = 2)
abline(h = mean(y[1 : (n/2)]), lwd = 3)
abline(h = mean(y[(n/2 + 1) : n]), lwd = 3)
fit <- lm(y ~ x + t)
abline(coef(fit)[1], coef(fit)[2], lwd = 3)
abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2], lwd = 3)
points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = "black", bg = "lightblue", cex = 2)
points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = "black", bg = "salmon", cex = 2)
```
non-significant effect when we ignore X, very significant effect when we include X.

### Some things to note in this simulation
* No marginal association between group status and Y.
* Strong adjusted relationship.
* Group status not related to X.
* There is lots of direct evidence for comparing red and blue
holding X fixed.


## Simulation 5
```{r, fig.height=5, fig.width=5, echo = FALSE, results='hide'}
n <- 100; t <- rep(c(0, 1), c(n/2, n/2)); x <- c(runif(n/2, -1, 1), runif(n/2, -1, 1));
beta0 <- 0; beta1 <- 2; tau <- 0; tau1 <- -4; sigma <- .2
y <- beta0 + x * beta1 + t * tau + t * x * tau1 + rnorm(n, sd = sigma)
plot(x, y, type = "n", frame = FALSE)
abline(lm(y ~ x), lwd = 2)
abline(h = mean(y[1 : (n/2)]), lwd = 3)
abline(h = mean(y[(n/2 + 1) : n]), lwd = 3)
fit <- lm(y ~ x + t + I(x * t))
abline(coef(fit)[1], coef(fit)[2], lwd = 3)
abline(coef(fit)[1] + coef(fit)[3], coef(fit)[2] + coef(fit)[4], lwd = 3)
points(x[1 : (n/2)], y[1 : (n/2)], pch = 21, col = "black", bg = "lightblue", cex = 2)
points(x[(n/2 + 1) : n], y[(n/2 + 1) : n], pch = 21, col = "black", bg = "salmon", cex = 2)
```

Y = beta_0 + beta_1 * T + beta_2 * X + beta_3 * T * X + e
there is not beta_1 treatment effect for this data. the outcome depends on X. 

---
## Discussion
### Some things to note from this simulation
* There is no such thing as a group effect here.
  * The impact of group reverses itself depending on X.
  * Both intercept and slope depends on group.
* Group status and X unrelated.
  * There's lots of information about group effects holding X fixed.
  
  
  
### Simulation 6
```{r, fig.height=5, fig.width=5, echo = FALSE, results='hide'}
p <- 1
n <- 100; x2 <- runif(n); x1 <- p * runif(n) - (1 - p) * x2
beta0 <- 0; beta1 <- 1; tau <- 4 ; sigma <- .01
y <- beta0 + x1 * beta1 + tau * x2 + rnorm(n, sd = sigma)
plot(x1, y, type = "n", frame = FALSE)
abline(lm(y ~ x1), lwd = 2)
co.pal <- heat.colors(n)
points(x1, y, pch = 21, col = "black", bg = co.pal[round((n - 1) * x2 + 1)], cex = 2)
```

---
### Do this to investigate the bivariate relationship
```
library(rgl)
plot3d(x1, x2, y)
```

On MacOS, rgl depends on XQuartz, which you can download from xquartz.org.
3 dimensional plots are difficult to work with, but we can look at the residuals instead.


---
### Residual relationship
```{r, fig.height=5, fig.width=5, echo = FALSE, results='hide'}
plot(resid(lm(x1 ~ x2)), resid(lm(y ~ x2)), frame = FALSE, col = "black", bg = "lightblue", pch = 21, cex = 2)
abline(lm(I(resid(lm(x1 ~ x2))) ~ I(resid(lm(y ~ x2)))), lwd = 2)
```

there is a very strong relationship leftover between x1 and y after taking out x2 effects.

---
## Discussion
### Some things to note from this simulation

* X1 unrelated to X2
* X2 strongly related to Y
* Adjusted relationship between X1 and Y largely unchanged
  by considering X2.
  * Almost no residual variability after accounting for X2.

---
## Some final thoughts
* Modeling multivariate relationships is difficult.
* Play around with simulations to see how the
  inclusion or exclusion of another variable can
  change analyses.
* The results of these analyses deal with the
impact of variables on associations.
  * Ascertaining mechanisms or cause are difficult subjects
    to be added on top of difficulty in understanding multivariate associations.
    
we haven't said which is the right model. best way to think about that is bringing in some subject matter expertise into your model exercise.


sometimes the treatment and the other regressor are highly related; merely because they are related is not interesting enough to adjust for x

we might have systolic blood pressure in the model, and i put diastolic blood pressure in a model as well, that of course makes the first one go away. but it's not interesting. i know they are highly correlated, why should i have both of them in the model?


# Residuals and Diagnostocs part

```{r}
data(swiss)
par(mfrow = c(2, 2))
fit <- lm(Fertility ~ ., data = swiss); plot(fit)
```

*Influential, high leverage and outlying points*

calling a point an outlier is vague.

outliers can be the result of spurious or real processes.

outliers can conform to the regression relationship.(i.e. being marginally outlying in X or Y, but not outlying given the regression relationship)

## List of Default Diagnostic Measures:
do ?influence.measures to see the full suite of influence measures in stats. the measures include:

* `rstandard` - standardized residuals, residuals devided by their standard deviations
* `rstudent` - standardized residuals, residuals divided by their standard deviations, where the ith data point was deleted in the calculation of the standard deviation for the residual to follow a t distribution

residuals are unit specific. they are not necessaryily comparable accross settings. it would be nice to have an outlier measure, like if a residual was bigger than 4 it is an outlier.. 

standardized residuals are handy, but look at them with respect to the cloud of data.(internally vs. externally standardized residuals)

* `hatvalues` - measures of leverage (how far your x value is from the collection of Xs.) this is hard to measure in a multivariate environment. *this is EXTREMELY useful in finding erroneous data entries.*

* `dffits` - change in the predicted response when the $i^{th}$ point is deleted in fitting the model.
(take out that data point, refit the model, then compare to whatever aspect of the model you're thinking about what happened between having the data point in and having data point out.)

  * `dfbetas` - change in individual coefficients when the $i^{th}$ point is deleted in fitting the model. (does the previous for slope coefficients)
  
  * `cooks.distance` - overall change in teh coefficients when the $i^{th}$ point is deleted.
  
  * `resid` - returns the ordinary residuals

  * `resid(fit) / (1 - hatvalues(fit))` where `fit` is the linear model fit returns the PRESS residuals, i.e. the leave one out cross validation residuals - the difference in the response and the predicted response at data point $i$, where it was not included in the model fitting.
  
## How to use all of these things

an important aspect of all these measures is that they probe your data to look at lack of influence, and whether or not something is an outlier in separate ways. they're diagnostics.

we are looking for systematic pattern. if they are present it means there's an error in the model.

in residual plots the main residual plot that everyone does always is to plot the residuals versus the fitted values. in these residual plots we're looking for any systematic patterns, if they exist, that's evidence of some kind of lack of fit. another plot is the residual QQ plot that's trying to ascertain normality. the plot of the leveraged values is just ascertaining that there are any points that have high leverage. you want to go look at those rows specifically. and then the influence measures get to the bottom line. if i remove this data point, how much does it impact the fitted model, the coefficients, etc..


## Case 1
```{r, fig.height=5, fig.width=5, echo=FALSE}
x <- c(10, rnorm(n)); y <- c(10, c(rnorm(n)))
plot(x, y, frame = FALSE, cex = 2, pch = 21, bg = "lightblue", col = "black")
abline(lm(y ~ x))            
```

* The point `c(10, 10)` has created a strong regression relationship where there shouldn't be one.

---
## Showing a couple of the diagnostic values
```{r}
fit <- lm(y ~ x)
round(dfbetas(fit)[1 : 10, 2], 3)
round(hatvalues(fit)[1 : 10], 3)
```
the 1st point is the c(10,10) point. orders of magnitue larger dfbeta than remaining points.

the hat value of that point is much larger than the hat values of other points.

we can obviously single this point out.

---
## Case 2
```{r, fig.height=5, fig.width=5, echo=FALSE}
x <- rnorm(n); y <- x + rnorm(n, sd = .3)
x <- c(5, x); y <- c(5, y)
plot(x, y, frame = FALSE, cex = 2, pch = 21, bg = "lightblue", col = "black")
fit2 <- lm(y ~ x)
abline(fit2)            
```
there's a clear regression relationship. theres a far point but it is on the regression line.


---
## Looking at some of the diagnostics
```{r, echo = TRUE}
round(dfbetas(fit2)[1 : 10, 2], 3)
round(hatvalues(fit2)[1 : 10], 3)
```
the dfbeta of the outlying point is still large, but nothing like the other case.

but if you look at the hatvalue, it has a much larger hatvalue than other points. because it is outside of the range of the x values. 

large leverage value but not large dfbeta value

---
## Example described by Stefanski TAS 2007 Vol 61.
```{r, fig.height=4, fig.width=4}
## Don't everyone hit this server at once.  Read the paper first.
dat <- read.table('http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/orly_owl_files/orly_owl_Lin_4p_5_flat.txt', header = FALSE)
pairs(dat)
```
residuals zoom in on potential errors in our model and data.


---
## Got our P-values, should we bother to do a residual plot?
```{r}
summary(lm(V1 ~ . -1, data = dat))$coef
```

---
## Residual plot
### P-values significant, O RLY?
```{r, fig.height=4, fig.width=4, echo = TRUE}
fit <- lm(V1 ~ . - 1, data = dat); plot(predict(fit), resid(fit), pch = '.')
```
without looking at the residuals we wouldn't have seen anything. looking at the residuals showed us the poor model fit. there's a pattern, and it's mocking us. xD


---
## Back to the Swiss data
```{r, fig.height = 5, fig.width = 5, echo=FALSE}
data(swiss); par(mfrow = c(2, 2))
fit <- lm(Fertility ~ . , data = swiss); plot(fit)
```

the first plot is the residuals vs the fitted values. the classic. check if there's any pattern.

the QQ plot is specifically designed to evalute normallity of the error terms. 

scale location plot is plotting the standardized residuals. comparable across experiments. a lot like the residuals plot, but you change the scale.

final plot is residuals vs legerage.

none of them here look inherently bad, but when you go through these things, you want to see the details of the specific points with high leverage.

# Model Selection

These lectures represents a challenging question: “How do we chose what variables to include in a regression model?”. Sadly, no single easy answer exists and the most reasonable answer would be “It depends.”. These concepts bleed into ideas of machine learning, which is largely focused on high dimensional variable selection and weighting. In the following lectures we cover some of the basics and, most importantly, the consequences of over- and under-fitting a model.

## Multivariable regression
* We have an entire class on prediction and machine learning, so we'll focus on modeling.
  * Prediction has a different set of criteria, needs for interpretability and standards for generalizability.
  * In modeling, our interest lies in parsimonious, interpretable representations of the data that enhance our understanding of the phenomena under study.
so if a variable in a regression model explains a tiny bit extra about a variation but really harms the interpretability, we're going to omit that variable, even if it really does explain a little bit of positive amount of variability.
  * A model is a lense through which to look at your data. (I attribute this quote to Scott Zeger)
any model that teaches you something about the data is true. but some cases when you want a super high level detail you might need a telescope.

  * Under this philosophy, what's the right model? Whatever model connects the data to a true, parsimonious statement about what you're studying.
  
* There are nearly uncontable ways that a model can be wrong, in this lecture, we'll focus on variable inclusion and exclusion.

* Like nearly all aspects of statistics, good modeling decisions are context dependent.

  * A good model for prediction versus one for studying mechanisms versus one for trying to establish causal effects may not be the same.

isin bu kismi uygulamayla oturan bir konu gibi. 

---
## The Rumsfeldian triplet

*There are known knowns. These are things we know that we know. There are known unknowns. That is to say, there are things that we know we don't know. But there are also unknown unknowns. There are things we don't know we don't know.* Donald Rumsfeld

In our context(finding and choosing regressors)
* (Known knowns) Regressors that we know we should check to include in the model and have.
* (Known Unknowns) Regressors that we would like to include in the model, but don't have.
* (Unknown Unknowns) Regressors that we don't even know about that we should have included in the model.


---
## General rules
* Omitting variables results in bias in the coeficients of interest - unless their regressors are uncorrelated with the omitted ones.
  * This is why we randomize treatments, it attempts to uncorrelate our treatment indicator with variables that we don't have to put in the model. 
  * (If there's too many unobserved confounding variables, even randomization won't help you.)

what happens if we omit a variable that we should have included? general result is that results in bias. one way to prevent that is try and randomize. if this other variable that you're omitting is uncorrelated with the variable that you're interested in, then it doesn't matter usually whether you include it or not. that's why AB testing and clinical trials are powerful.

* Including variables that we shouldn't have increases standard errors of the regression variables. it doesn't create bias, but it inflates actual standard errors.

  * Actually, including any new variables increasese (actual, not estimated) standard errors of other regressors. So we don't want to idly throw variables into the model.
  
* The model must tend toward perfect fit as the number of non-redundant regressors approaches $n$.

* $R^2$ increases monotonically as more regressors are included.

* The SSE decreases monotonically as more regressors are included.


---
## Plot of $R^2$ versus $n$
For simulations as the number of variables included equals increases to $n=100$. 
No actual regression relationship exist in any simulation
```{r, fig.height=5, fig.width=5, echo=FALSE}
 n <- 100
plot(c(1, n), 0 : 1, type = "n", frame = FALSE, xlab = "p", ylab = "R^2")
r <- sapply(1 : n, function(p)
      {
        y <- rnorm(n); x <- matrix(rnorm(n * p), n, p)
        summary(lm(y ~ x))$r.squared 
      }
    )
lines(1 : n, r, lwd = 2)
abline(h = 1)
```


## model selection part 2




---
## Variance inflation
```{r, echo = TRUE}
n <- 100; nosim <- 1000
x1 <- rnorm(n); x2 <- rnorm(n); x3 <- rnorm(n); 
betas <- sapply(1 : nosim, function(i){
  y <- x1 + rnorm(n, sd = .3)
  c(coef(lm(y ~ x1))[2], 
    coef(lm(y ~ x1 + x2))[2], 
    coef(lm(y ~ x1 + x2 + x3))[2])
})
round(apply(betas, 1, sd), 5)
# the outcome only depends on x1, but if you include x2 and x3 in the regression, the standard error of the coefficients increase. this is what happens when you include gereksiz regressors.

# the inflation here is negligable because x2 and x3 is not related to x1.
```

## !!!

The variance inflation occurs on the actual standard errors, not the estimated standard errors. It's actually a little complicated, in that if I put another variable into a regression model, it doesn't necessarily inflate the observed standard error. And the reason is because the variance is estimated rather than the actual variance. And that has a kind of conflicting property.So you don't necessarily see the variance go up when you include an unnecessary regressor into a regression model, you have to look at it through simulation.

---
## Variance inflation
```{r, echo = TRUE}
n <- 100; nosim <- 1000
x1 <- rnorm(n); x2 <- x1/sqrt(2) + rnorm(n) /sqrt(2)
x3 <- x1 * 0.95 + rnorm(n) * sqrt(1 - 0.95^2); 
betas <- sapply(1 : nosim, function(i){
  y <- x1 + rnorm(n, sd = .3)
  c(coef(lm(y ~ x1))[2], 
    coef(lm(y ~ x1 + x2))[2], 
    coef(lm(y ~ x1 + x2 + x3))[2])
})
round(apply(betas, 1, sd), 5)
# when the unnecessary regressors depend heavily on x1, then you have a very high inflation of variation.
```

if the variable that you include is highly correlated to the thing you're interested in, you're going to inflate the variance more.
 


---
## Variance inflation factors
* Notice variance inflation was much worse when we included a variable that
was highly related to `x1`. 
* We don't know $\sigma$, so we can only estimate the increase in the actual standard error of the coefficients for including a regressor.
* However, $\sigma$ drops out of the relative standard errors. If one sequentially adds variables, one can check the variance (or sd) inflation for including each one.
* When the other regressors are actually orthogonal to the regressor of interest, then there is no variance inflation.
* The variance inflation factor (VIF) is the increase in the variance for the ith regressor compared to the ideal setting where it is orthogonal to the other regressors.
  * (The square root of the VIF is the increase in the sd ...)
* Remember, variance inflation is only part of the picture. We want to include certain variables, even if they dramatically inflate our variance. 

---
## Revisting our previous simulation
```{r, echo = TRUE}
##doesn't depend on which y you use,
y <- x1 + rnorm(n, sd = .3)
a <- summary(lm(y ~ x1))$cov.unscaled[2,2]
c(summary(lm(y ~ x1 + x2))$cov.unscaled[2,2],
  summary(lm(y~ x1 + x2 + x3))$cov.unscaled[2,2]) / a
temp <- apply(betas, 1, var); temp[2 : 3] / temp[1]
```
---
## Swiss data
```{r}
data(swiss); 
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
a <- summary(fit1)$cov.unscaled[2,2]
fit2 <- update(fit, Fertility ~ Agriculture + Examination)
fit3 <- update(fit, Fertility ~ Agriculture + Examination + Education)
  c(summary(fit2)$cov.unscaled[2,2],
    summary(fit3)$cov.unscaled[2,2]) / a 
```

---
## Swiss data VIFs, 
```{r}
library(car) #for vif() function
fit <- lm(Fertility ~ . , data = swiss)
vif(fit) # variance inflation factors
sqrt(vif(fit)) #I prefer sd 
```
this means the variation of Agriculture effect is double of what it would be if it were orthogonal to all the other regressors. Examination and Education are particularly higher, as we know they are highly correlated. 


---
## What about residual variance estimation?
* Assuming that the model is linear with additive iid errors (with finite variance), we can mathematically describe the impact of omitting necessary variables or including unnecessary ones.
  * If we underfit the model, the variance estimate is biased. 
  * If we correctly or overfit the model, including all necessary covariates and/or unnecessary covariates, the variance estimate is unbiased.
    * However, the variance of the variance is larger if we include unnecessary variables.

---
## Covariate model selection
* Automated covariate selection is a difficult topic. It depends heavily on how rich of a covariate space one wants to explore. 
  * The space of models explodes quickly as you add interactions and polynomial terms. 
* In the prediction class, we'll cover many modern methods for traversing large model spaces for the purposes of prediction.
* Principal components or factor analytic models on covariates are often useful for reducing complex covariate spaces.
* Good design can often eliminate the need for complex model searches at analyses; though often control over the design is limited.
* If the models of interest are nested and without lots of parameters differentiating them, it's fairly uncontroversial to use nested likelihood ratio tests. (Example to follow.)
* My favoriate approach is as follows. Given a coefficient that I'm interested in, I like to use covariate adjustment and multiple models to probe that effect to evaluate it for robustness and to see what other covariates knock it out.  This isn't a terribly systematic approach, but it tends to teach you a lot about the the data as you get your hands dirty.

---
## How to do nested model testing in R
```{r}
fit1 <- lm(Fertility ~ Agriculture, data = swiss)
fit3 <- update(fit, Fertility ~ Agriculture + Examination + Education)
fit5 <- update(fit, Fertility ~ Agriculture + Examination + Education + Catholic + Infant.Mortality)
anova(fit1, fit3, fit5)
```

degrees of freedom 
residual sums of squares 
the excess degrees of freedom : of going from model 1 to model 2
F : S statistic
Pr(>F) : P value

here it shows that yes, the inclusion of education and examination information appears to be necessary, and yes, the inclusion of religion and infant mortality appears to be necessary.

# Swirl 

During this week of the course you should complete the following lessons in the Regression Models swirl course:

MultiVar Examples2
MultiVar Examples3
Residuals Diagnostics and Variation

## Multivar Examples 2


Without an intercept (reference group) the tests are whether the expected counts
| (the groups means) are different from zero. Which spray has the least significant
| result?

1: sprayF
2: sprayC
3: sprayA
4: sprayB

Selection: 2

| That's the answer I was looking for.

  |====================================================                       |  70%
| Clearly, which level is first is important to the model. If you wanted a different
| reference group, for instance, to compare sprayB to sprayC, you could refit the
| model with a different reference group.

 The R function relevel does precisely this. It re-orders the levels of a factor.
| We'll do this now. We'll call relevel with two arguments. The first is the factor,
| in this case InsectSprays$spray, and the second is the level that we want to be
| first, in this case "C". Store the result in a new variable spray2.





